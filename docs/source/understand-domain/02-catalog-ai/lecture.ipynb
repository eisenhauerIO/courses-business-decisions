{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Catalog AI\n",
    "\n",
    "This notebook implements a toy version of the **Catalog AI** system discussed in [Addressing Gen AI's Quality Control Problem](https://hbr.org/2025/09/addressing-gen-ais-quality-control-problem) (Harvard Business Review)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "![Harvard Business Review article header showing title Addressing Gen AI's Quality Control Problem](article.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {},
   "source": [
    "Generative AI systems can produce vast numbers of candidate product data improvements, but at scale the central problem is not generation—it is quality control and learning. When Amazon began automating product data improvements using large language models to enhance product `title`, `description`, and `features`, most generated outputs were initially unreliable, and only a small fraction improved business outcomes like conversion and revenue. The solution was not human review, but a self-learning system that combines automated guardrails to filter low-quality outputs, experimentation to measure impact on shopper behavior, and feedback loops that use experimental results to guide future generation. The system can test millions of product data hypotheses, discard most of them, and continuously learn from the few that work. This notebook introduces a simplified, toy version of that idea: how to build a **Learn · Decide · Repeat** loop that turns large-scale product data analysis into action and learning over time."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3",
   "metadata": {},
   "source": [
    "![Online Retail Simulator architecture diagram showing the Learn Decide Repeat workflow](online-retail-simulator.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4",
   "metadata": {},
   "source": [
    "We use the **[Online Retail Simulator](https://github.com/eisenhauerIO/tools-catalog-generator)** to demonstrate three capabilities. First, we generate product `title`, `description`, and `features` using a local LLM via [**Ollama**](https://ollama.com/) with customizable prompts. Using simulated data, we then apply causal impact measurement to establish whether these content improvements actually drive shopper behavior and sales, and to identify which positioning strategy—luxury versus budget—produces the strongest business outcomes. This illustrates how impact measurement guides data-driven content strategy selection."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5",
   "metadata": {},
   "source": [
    "Before running this notebook, execute the setup script `setup_catalog_ai.sh` to install and configure Ollama:\n",
    "\n",
    "```bash\n",
    "bash setup_catalog_ai.sh\n",
    "```\n",
    "\n",
    "The script installs the Online Retail Simulator package and sets up Ollama with the `llama3.2` model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard library\n",
    "import inspect\n",
    "from operator import itemgetter\n",
    "\n",
    "# Third-party packages\n",
    "from IPython.display import Code\n",
    "\n",
    "# Local imports\n",
    "from online_retail_simulator import simulate, load_job_results, enrich\n",
    "from online_retail_simulator.simulate.product_details_ollama import simulate_product_details_ollama\n",
    "from support import (\n",
    "    print_product_details,\n",
    "    plot_treatment_effect,\n",
    "    plot_positioning_comparison,\n",
    "    print_positioning_comparison,\n",
    ")\n",
    "\n",
    "NUM_SAMPLE_PRODUCTS = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7",
   "metadata": {},
   "source": [
    "## Generate Product Content with Ollama"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8",
   "metadata": {},
   "source": [
    "The [**Ollama**](https://ollama.com/) backend uses a local LLM to generate unique, contextually-aware product `title`, `description`, and `features`. Custom prompts let you control the tone and style—enabling different **positioning strategies** for the same products. We'll start by generating product content using a value-focused positioning strategy that emphasizes practicality and affordability."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9",
   "metadata": {},
   "source": [
    "### Generate Baseline Product Content\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10",
   "metadata": {},
   "source": [
    "We start by generating a product catalog with basic **characteristics** using the simulator. These base products contain `product_identifier`, `category`, and `price`—the foundation upon which we'll generate detailed product content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start with base products before adding AI-generated content\n",
    "job_info = simulate(\"config_simulation.yaml\")\n",
    "products, metrics = itemgetter(\"products\", \"metrics\")(load_job_results(job_info))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select sample products for demonstration\n",
    "sample_products = products[[\"product_identifier\", \"category\", \"price\"]].head(NUM_SAMPLE_PRODUCTS)\n",
    "sample_products"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13",
   "metadata": {},
   "source": [
    "### Generate Product Content with AI\n",
    "\n",
    "We'll use a value-focused prompt template that guides the LLM to generate practical, budget-friendly product content. The prompt emphasizes functionality, value, and affordability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the budget positioning prompt template\n",
    "! cat \"prompt_budget.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate budget-positioned product content using Ollama LLM\n",
    "budget_products = simulate_product_details_ollama(sample_products, prompt_path=\"prompt_budget.txt\")\n",
    "print_product_details(budget_products, \"Budget\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16",
   "metadata": {},
   "source": [
    "To understand how the generation works under the hood, let's examine the `simulate_product_details_ollama()` function. It batches products, formats them as JSON, injects them into the prompt template, and sends requests to the local Ollama API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "Code(inspect.getsource(simulate_product_details_ollama), language=\"python\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18",
   "metadata": {},
   "source": [
    "## Measure Impact on Sales\n",
    "\n",
    "Generating content is only the first step. To build a self-learning system, we need to measure whether improved product content actually drives shopper behavior and business outcomes. Using the simulator's enrichment capability, we can inject known treatment effects and validate our measurement approach against ground truth.\n",
    "\n",
    "The simulator lets us inject known treatment effects through the **enrichment** layer. We specify the `effect_size` (percentage boost in sales), `enrichment_start` date, and which products receive treatment. This creates a controlled experiment where we know the true causal impact."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": [
    "!cat \"config_enrichment_budget.yaml\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20",
   "metadata": {},
   "source": [
    "Now we'll use the budget prompt to generate enhanced product content and simultaneously inject a 75% sales boost. This simulates what would happen if improved product content actually drove higher conversion and revenue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply enrichment with budget prompt\n",
    "enriched_job = enrich(\"config_enrichment_budget.yaml\", job_info)\n",
    "enriched_results = load_job_results(enriched_job)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22",
   "metadata": {},
   "source": [
    "The enriched dataset contains the same metrics as before (`impressions`, `visits`, `cart_adds`, `ordered_units`, `revenue`) but now with measurable treatment effects. Let's visualize how revenue changes before and after the enrichment date."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23",
   "metadata": {},
   "outputs": [],
   "source": [
    "enriched_metrics = enriched_results[\"enriched\"]\n",
    "plot_treatment_effect(metrics, enriched_metrics, \"2024-11-15\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24",
   "metadata": {},
   "source": [
    "The visualization shows clear evidence of the treatment effect—revenue increases after the enrichment date. The AI-generated product content appears to drive measurable business impact. But is this the best we can do? What if the same products were positioned differently?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25",
   "metadata": {},
   "source": [
    "## Compare Positioning Strategies\n",
    "\n",
    "The same product can be positioned in radically different ways depending on the prompt. We've focused on value-driven, budget-friendly positioning—but what if we tried a luxury approach instead? In production systems, you'll generate thousands of content variations using different prompts, tones, and strategies. The question is: which prompt produces content that drives the strongest business impact? To answer this, we need to run parallel experiments and compare measured outcomes across positioning strategies.\n",
    "\n",
    "Let's explore a completely different content strategy: luxury positioning. Instead of emphasizing value and practicality, luxury positioning focuses on premium quality, exclusivity, and aspirational appeal. The prompt template guides the LLM to generate elegant, sophisticated product descriptions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26",
   "metadata": {},
   "outputs": [],
   "source": [
    "! cat \"prompt_luxury.txt\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27",
   "metadata": {},
   "source": [
    "Let's see how the same products generate completely different content when using luxury positioning versus budget positioning. We'll apply both prompts to the same base products for comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate luxury-positioned products using the same base products\n",
    "luxury_products = simulate_product_details_ollama(sample_products, prompt_path=\"prompt_luxury.txt\")\n",
    "\n",
    "# Display side-by-side comparison\n",
    "print_product_details(budget_products, \"Budget Positioning\")\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print_product_details(luxury_products, \"Luxury Positioning\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29",
   "metadata": {},
   "source": [
    "### Measure Luxury Impact\n",
    "\n",
    "Now let's run a parallel experiment using luxury positioning to see how premium-focused content performs compared to budget positioning. We'll inject a 50% treatment effect to measure the business impact of this alternative strategy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the luxury enrichment configuration\n",
    "! cat \"config_enrichment_luxury.yaml\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply luxury enrichment\n",
    "luxury_job = enrich(\"config_enrichment_luxury.yaml\", job_info)\n",
    "luxury_results = load_job_results(luxury_job)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32",
   "metadata": {},
   "source": [
    "### Compare Business Outcomes\n",
    "\n",
    "Now we can compare the business impact of both positioning strategies. Which approach drives stronger results: budget-focused practicality or luxury-focused exclusivity? The data will tell us which content strategy to deploy at scale."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare positioning strategies\n",
    "stats = plot_positioning_comparison(metrics, enriched_results[\"enriched\"], luxury_results[\"enriched\"], \"2024-11-15\")\n",
    "\n",
    "# Print comparison results\n",
    "print_positioning_comparison(stats)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34",
   "metadata": {},
   "source": [
    "In this simplified demonstration, we've shown how different prompts generate different content and drive different business outcomes. This illustrates the **Learn · Decide · Repeat** loop as the foundation of self-learning AI systems at scale."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35",
   "metadata": {},
   "source": [
    "## Limitations and Caveats\n",
    "\n",
    "This demonstration uses a simplified, toy version of a self-learning content generation system. Several important limitations should be considered:\n",
    "\n",
    "- **Simulated treatment effects**: The business impact shown here is injected via configuration, not measured from actual shopper behavior. In production, treatment effects must be estimated through proper experimentation.\n",
    "- **Small sample size**: We demonstrate with only a few products. Real systems process millions of products, requiring robust infrastructure and careful attention to statistical power.\n",
    "- **Known ground truth**: The simulator provides known effect sizes for validation. In practice, true causal effects are never directly observable and must be inferred through careful experimental design.\n",
    "- **No guardrails shown**: Production systems require automated quality filters to prevent low-quality or harmful content from reaching customers. This notebook focuses on the measurement loop, not the filtering layer.\n",
    "- **Local LLM limitations**: The `llama3.2` model used here is smaller and less capable than production-grade models. Content quality varies significantly across model sizes and architectures."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.2"
  },
  "nbsphinx": {
   "execute": "never"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
