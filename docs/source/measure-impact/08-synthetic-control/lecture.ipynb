{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Synthetic Control\n",
    "\n",
    "> **Reference:** *Causal Inference: The Mixtape*, Chapter 10: Synthetic Control (pp. 469–516)\n",
    "\n",
    "This lecture introduces the synthetic control method for estimating causal effects in comparative case studies with panel data. We apply these concepts using the [Online Retail Simulator](https://github.com/eisenhauerIO/tools-catalog-generator) to answer: **Can synthetic control isolate a campaign's effect from common time trends that affect all products?**\n",
    "\n",
    "Each application lecture follows the same five-step workflow. We frame a causal question in a business context, simulate data with known ground truth using the Online Retail Simulator, measure the treatment effect — first with a naive approach, then with a causal method — using the Impact Engine, evaluate how well each method recovers the truth, and tune the method's parameters to understand how configuration choices affect the reliability of causal estimates.\n",
    "\n",
    "<img src=\"../../_static/lecture-structure.svg\" alt=\"Course workflow: Business Context, Simulate Data, Measure Impact, Evaluate Performance, Tune Parameters\" style=\"display: block; margin: 1em auto; max-width: 800px;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part I: Theory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {},
   "source": [
    "## 1. The Comparative Case Study\n",
    "\n",
    "Causal inference often involves evaluating the effect of a policy or intervention applied to a **single unit**—a country, state, firm, or product. When only one unit is treated, traditional methods face a fundamental challenge: there is no randomization and the \"sample size\" of treated units is exactly one.\n",
    "\n",
    "The traditional approach is a **comparative case study**: compare the affected unit to one or more \"comparison\" units not exposed to the intervention. For example, Card (1990) studied the 1980 Mariel Boatlift—which brought 125,000 Cuban immigrants to Miami—by comparing Miami's labor market to a set of comparison cities.\n",
    "\n",
    "The problem with traditional comparative case studies is **subjectivity in selecting comparison units**. Different researchers might choose different comparison groups and reach different conclusions. Is Atlanta a better comparison for Miami, or is Houston? The answer often depends on the researcher's judgment, which introduces an element of arbitrariness.\n",
    "\n",
    "The **synthetic control method** (Abadie and Gardeazabal 2003; Abadie, Diamond, and Hainmueller 2010) provides a systematic, data-driven approach to this problem. Rather than selecting a single comparison unit, it constructs a *synthetic* comparison as a weighted average of multiple untreated units."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3",
   "metadata": {},
   "source": [
    "## 2. The Synthetic Control Estimator\n",
    "\n",
    "### Setup\n",
    "\n",
    "Suppose we observe $J + 1$ units over $T$ time periods. Unit $j = 1$ receives a treatment or intervention at time $T_0 + 1$. The remaining $J$ units ($j = 2, \\ldots, J+1$) are untreated and form the **donor pool**.\n",
    "\n",
    "Let $Y_{jt}^N$ denote the potential outcome of unit $j$ at time $t$ in the absence of treatment, and $Y_{jt}^I$ the potential outcome under treatment. The causal effect of the intervention on the treated unit at time $t > T_0$ is:\n",
    "\n",
    "$$\\alpha_{1t} = Y_{1t}^I - Y_{1t}^N$$\n",
    "\n",
    "We observe $Y_{1t}^I$ directly (the treated unit's actual outcome after treatment). The challenge is estimating $Y_{1t}^N$—what the treated unit's outcome *would have been* without the intervention.\n",
    "\n",
    "### The Synthetic Control\n",
    "\n",
    "The synthetic control method estimates $Y_{1t}^N$ as a weighted average of the donor units' outcomes:\n",
    "\n",
    "$$\\hat{Y}_{1t}^N = \\sum_{j=2}^{J+1} w_j^* Y_{jt}$$\n",
    "\n",
    "where the weights $W^* = (w_2^*, \\ldots, w_{J+1}^*)'$ satisfy:\n",
    "\n",
    "- **Non-negativity**: $w_j \\geq 0$ for all $j$\n",
    "- **Sum to one**: $\\sum_{j=2}^{J+1} w_j = 1$\n",
    "\n",
    "These constraints ensure the synthetic control lies in the **convex hull** of the donor pool—it is an interpolation, never an extrapolation.\n",
    "\n",
    "The estimated treatment effect at each post-treatment period is:\n",
    "\n",
    "$$\\hat{\\alpha}_{1t} = Y_{1t} - \\sum_{j=2}^{J+1} w_j^* Y_{jt}$$\n",
    "\n",
    "### Controlling for Unobserved Factors\n",
    "\n",
    "Methods like matching and regression rely on **selection on observables**—they require all confounders to be measured. In comparative case studies, treatment often depends on unobserved factors (political conditions, strategic priorities) that these methods cannot address.\n",
    "\n",
    "Synthetic control rests on a different identification strategy. Abadie, Diamond, and Hainmueller (2010) model untreated outcomes as:\n",
    "\n",
    "$$Y_{jt}^N = \\delta_t + \\boldsymbol{\\theta}_t \\boldsymbol{\\mu}_j + \\varepsilon_{jt}$$\n",
    "\n",
    "where $\\delta_t$ is a common time effect, $\\boldsymbol{\\theta}_t$ is a vector of unobserved common factors, and $\\boldsymbol{\\mu}_j$ captures unit $j$'s factor loadings (its response to those factors). If a weighted combination of donor units reproduces the treated unit's pre-treatment trajectory, it implicitly matches on the factor loadings $\\boldsymbol{\\mu}_j$—and therefore controls for the unobserved factors $\\boldsymbol{\\theta}_t$ in the post-treatment period.\n",
    "\n",
    "This result depends critically on the **length of the pre-treatment period**. With few pre-treatment periods, many donor combinations could match the treated trajectory by coincidence. As the pre-treatment period grows, the matching becomes increasingly informative about shared factor structure."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4",
   "metadata": {},
   "source": [
    "## 3. Choosing Weights\n",
    "\n",
    "The optimal weights are chosen to make the synthetic control match the treated unit as closely as possible in the pre-treatment period. Let $k$ denote the number of pre-treatment characteristics used for matching, and define:\n",
    "\n",
    "- $X_1$ be a $(k \\times 1)$ vector of pre-treatment characteristics for the treated unit\n",
    "- $X_0$ be a $(k \\times J)$ matrix of the same characteristics for the donor units\n",
    "\n",
    "The characteristics in $X$ typically include pre-treatment outcome values (or averages over pre-treatment subperiods) and other predictors of the outcome.\n",
    "\n",
    "The optimal weights minimize:\n",
    "\n",
    "$$W^* = \\arg\\min_W \\|X_1 - X_0 W\\|_V = \\arg\\min_W \\sqrt{(X_1 - X_0 W)' V (X_1 - X_0 W)}$$\n",
    "\n",
    "subject to $w_j \\geq 0$ and $\\sum w_j = 1$.\n",
    "\n",
    "### The Role of $V$\n",
    "\n",
    "The matrix $V$ is a $(k \\times k)$ positive semidefinite matrix that reflects the relative importance of different pre-treatment characteristics. It can be:\n",
    "\n",
    "| Approach | Description |\n",
    "|----------|-------------|\n",
    "| **Researcher-specified** | Set $V$ based on domain knowledge about which characteristics matter most |\n",
    "| **Data-driven** | Choose $V$ to minimize the mean squared prediction error (MSPE) of the outcome in the pre-treatment period |\n",
    "| **Diagonal** | Restrict $V$ to be diagonal, where each entry reflects one characteristic's importance |\n",
    "\n",
    "In practice, the data-driven approach is most common: a nested optimization selects $V$ in an outer loop to minimize pre-treatment MSPE, and given $V$, the inner loop solves for the optimal weights $W^*$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5",
   "metadata": {},
   "source": [
    "## 4. Advantages Over Regression\n",
    "\n",
    "A natural alternative to synthetic control is to regress the treated unit's outcome on the donor units' outcomes using ordinary least squares. Why prefer synthetic control?\n",
    "\n",
    "| Criterion | Synthetic Control | Regression |\n",
    "|-----------|-------------------|------------|\n",
    "| **Interpolation** | Weights are non-negative and sum to 1 $\\rightarrow$ synthetic control lies within the convex hull of donors | Coefficients are unrestricted $\\rightarrow$ can extrapolate beyond the data |\n",
    "| **Transparency** | Weights reveal exactly which units contribute and how much | Coefficients mix unit contributions with functional form |\n",
    "| **Overfitting** | Convexity constraint acts as implicit regularization | OLS with many predictors can overfit pre-treatment data while predicting post-treatment poorly |\n",
    "| **Design focus** | Constructed entirely from pre-treatment data, separating design from analysis | Same specification used for estimation, inviting specification search |\n",
    "\n",
    "The interpolation property is particularly important. When the number of potential control units is large relative to the number of pre-treatment periods, OLS will perfectly fit the pre-treatment data (overfitting) but may perform poorly out of sample. The convexity constraint of the synthetic control acts as a natural regularizer, forcing the method to find a *meaningful* combination of donors rather than an arbitrary one."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6",
   "metadata": {},
   "source": [
    "## 5. Inference\n",
    "\n",
    "Standard inference methods—t-tests, confidence intervals from asymptotic theory—are not appropriate for synthetic control because the treated sample size is one. Instead, Abadie, Diamond, and Hainmueller (2010) propose **placebo tests** based on permutation inference.\n",
    "\n",
    "### Placebo-in-Space Tests\n",
    "\n",
    "The procedure is:\n",
    "\n",
    "1. For each unit $j$ in the donor pool, pretend it was treated and fit a synthetic control using the remaining units as donors\n",
    "2. Compute the **gap** (difference between actual and synthetic) for each placebo unit in the post-treatment period\n",
    "3. Compare the treated unit's gap to the distribution of placebo gaps\n",
    "\n",
    "If the treated unit's gap is unusually large relative to the placebo distribution, we have evidence of a genuine treatment effect.\n",
    "\n",
    "Placebo tests also serve as a check on the unobservables assumption from Section 2. If the treated unit's post-treatment divergence were driven by an unobserved confounder rather than the treatment, then donor units—exposed to the same unobserved factors but not treated—should show comparable gaps when subjected to placebo analysis. A treated unit whose gap is extreme relative to the placebo distribution provides evidence that the effect is genuine, not an artifact of differential exposure to unobserved shocks.\n",
    "\n",
    "### RMSPE Ratios\n",
    "\n",
    "To formalize the comparison, compute the ratio of post-treatment to pre-treatment root mean squared prediction error for each unit:\n",
    "\n",
    "$$r_j = \\frac{\\text{RMSPE}_j^{\\text{post}}}{\\text{RMSPE}_j^{\\text{pre}}}$$\n",
    "\n",
    "where $\\text{RMSPE}_j^{\\text{post}}$ measures the post-treatment gap and $\\text{RMSPE}_j^{\\text{pre}}$ measures the pre-treatment fit quality.\n",
    "\n",
    "The ratio adjusts for the fact that some placebo units may have poor pre-treatment fit (large $\\text{RMSPE}^{\\text{pre}}$), which would inflate their post-treatment gaps even without any real effect.\n",
    "\n",
    "### Exact p-Value\n",
    "\n",
    "The treated unit's rank among all $J + 1$ RMSPE ratios gives an **exact p-value**:\n",
    "\n",
    "$$p = \\frac{\\text{Rank of treated unit's } r_1 \\text{ among all } r_j}{J + 1}$$\n",
    "\n",
    "For example, if the treated unit has the highest RMSPE ratio among 20 units (1 treated + 19 donors), the p-value is $1/20 = 0.05$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7",
   "metadata": {},
   "source": [
    "## 6. Practical Considerations\n",
    "\n",
    "### Donor Pool Selection\n",
    "\n",
    "The donor pool should contain units that are **plausible comparisons**—units that could reasonably approximate the treated unit's trajectory in the absence of treatment. Exclude units that:\n",
    "\n",
    "- Experienced similar interventions or major idiosyncratic shocks during the study period\n",
    "- Are structurally very different from the treated unit (different scale, different market)\n",
    "\n",
    "Including inappropriate donors adds noise without improving the counterfactual estimate.\n",
    "\n",
    "### Pre-Treatment Fit Quality\n",
    "\n",
    "The credibility of the synthetic control depends on how well it tracks the treated unit in the pre-treatment period. A large **pre-treatment MSPE** is a warning sign: if the method cannot reproduce the treated unit's trajectory before treatment, there is little reason to trust its counterfactual prediction after treatment. A poor fit may indicate that the donor pool does not contain units comparable to the treated unit on unobserved dimensions—undermining the method's ability to control for selection on unobservables.\n",
    "\n",
    "As a rule of thumb, the pre-treatment fit should be evaluated both visually (does the synthetic line closely track the treated line?) and quantitatively (is the MSPE small relative to the outcome's variance?).\n",
    "\n",
    "### When Synthetic Control Works Best\n",
    "\n",
    "The method is most effective when:\n",
    "\n",
    "- The pre-treatment period is long enough to reveal the outcome's dynamics—a requirement for the method's ability to control for unobserved confounders (Abadie, Diamond, and Hainmueller 2010)\n",
    "- The donor pool contains units whose weighted combination can reproduce the treated unit's trajectory\n",
    "- The treated unit's outcome lies within the range of donor outcomes (convex hull condition)\n",
    "- There are no large, idiosyncratic shocks to the treated unit in the pre-treatment period\n",
    "\n",
    "When these conditions hold, matching on pre-treatment outcomes implicitly controls for heterogeneous responses to unobserved confounders—the key identification result discussed in Section 2."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part II: Application\n",
    "\n",
    "In Part I we developed the theory of synthetic control: a weighted average of **donor pool** units constructs a transparent counterfactual for a single treated unit, with weights constrained to the **convex hull** of donor outcomes. Inference relies on **placebo tests** and **RMSPE ratios** rather than standard errors.\n",
    "\n",
    "We now apply this method to a product-level panel from our online retail simulation. The simulator provides both potential outcomes—a \"god's eye view\" that we would not have in real data—enabling numerical verification of the theoretical results from Part I. A subset of products receives a content optimization campaign starting on a specific date. We observe daily revenue for all products before and after the campaign, then use synthetic control to recover the causal effect for a single showcase product.\n",
    "\n",
    "**Can synthetic control isolate the campaign's effect from common time trends that affect all products?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Third-party packages\n",
    "import pandas as pd\n",
    "from impact_engine_measure import evaluate_impact, load_results\n",
    "from online_retail_simulator import enrich, load_job_results, simulate\n",
    "\n",
    "# Local imports\n",
    "from support import (\n",
    "    build_panel,\n",
    "    compute_ground_truth_att,\n",
    "    plot_average_fit,\n",
    "    plot_gap,\n",
    "    plot_method_comparison,\n",
    "    plot_treated_vs_synthetic,\n",
    "    plot_weights,\n",
    "    write_sc_config,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "import inspect\n",
    "\n",
    "from IPython.display import Code\n",
    "\n",
    "Code(inspect.getsource(build_panel), language=\"python\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "Code(inspect.getsource(compute_ground_truth_att), language=\"python\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12",
   "metadata": {},
   "source": [
    "## 1. Business Context\n",
    "\n",
    "An e-commerce company runs a **content optimization campaign** on 50 products (~10% of its catalog). The campaign improves product listings' images, descriptions, and search metadata, starting on a specific date. The company observes daily revenue for all 500 products—both the 50 treated and 450 untreated—before and after the campaign launch.\n",
    "\n",
    "The company wants to know: **how much additional daily revenue did the campaign generate?**\n",
    "\n",
    "The challenge is that revenue fluctuates over time for all products due to seasonal patterns, market conditions, and promotional cycles. A simple before-after comparison for a treated product would confound the campaign's effect with these common time trends. The synthetic control method addresses this by constructing a counterfactual from the untreated products' trajectories.\n",
    "\n",
    "| Variable | Notation | Description |\n",
    "|----------|----------|-------------|\n",
    "| Treated units | $j \\in \\mathcal{T}$ | 50 products receiving content optimization |\n",
    "| Donor pool | $j \\notin \\mathcal{T}$ | ~450 untreated products in the catalog |\n",
    "| Outcome | $Y_{jt}$ | Daily revenue for product $j$ at time $t$ |\n",
    "| Treatment time | $T_0$ | Campaign launch date |\n",
    "| Treatment effect | $\\alpha_{jt}$ | Additional daily revenue caused by the campaign |\n",
    "\n",
    "Because we control the data generation process, we observe both each product's actual revenue and what it *would have earned* without the campaign. This \"god's eye view\" lets us compute the true treatment effect and measure how closely each estimation method recovers it. We demonstrate synthetic control on a single **showcase product**, then examine aggregate tracking across treated and control groups."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13",
   "metadata": {},
   "source": [
    "### The Assignment Mechanism\n",
    "\n",
    "The 50 treated products are selected for the campaign based on their content quality scores—products with the weakest listings are prioritized for optimization. This creates **negative selection bias**: treated products tend to have lower baseline revenue than untreated products, even before any campaign effect.\n",
    "\n",
    "At the same time, all 500 products share a **common upward time trend** in revenue driven by seasonal patterns and market-wide growth. This trend affects treated and untreated products equally but contaminates any before-after comparison for a single product.\n",
    "\n",
    "A naive before-after estimator for a single treated product captures both the campaign's true effect *and* the shared time trend, biasing the estimate upward. The synthetic control method addresses this by constructing a weighted combination of untreated products whose pre-treatment trajectory matches the treated product. Because the donor units experience the same time trends but not the campaign, their weighted average \"subtracts out\" the common trajectory—isolating the causal effect of content optimization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "! cat \"config_simulation.yaml\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run simulation\n",
    "job_info = simulate(\"config_simulation.yaml\")\n",
    "metrics = load_job_results(job_info)[\"metrics\"]\n",
    "\n",
    "print(f\"Metrics records: {len(metrics)}\")\n",
    "print(f\"Unique products: {metrics['product_identifier'].nunique()}\")\n",
    "print(f\"Date range: {metrics['date'].min()} to {metrics['date'].max()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "! cat \"config_enrichment.yaml\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply enrichment: 50% quantity boost on 10% of products\n",
    "enriched_job = enrich(\"config_enrichment.yaml\", job_info)\n",
    "results = load_job_results(enriched_job)\n",
    "enriched_metrics = results[\"enriched\"]\n",
    "potential_outcomes = results[\"potential_outcomes\"]\n",
    "\n",
    "n_treated = enriched_metrics[enriched_metrics[\"enriched\"]][\"product_identifier\"].nunique()\n",
    "n_control = enriched_metrics[~enriched_metrics[\"enriched\"]][\"product_identifier\"].nunique()\n",
    "print(f\"Enriched records: {len(enriched_metrics)}\")\n",
    "print(f\"Treated products: {n_treated}\")\n",
    "print(f\"Control products: {n_control}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build analysis panel with common time trend\n",
    "TREATMENT_DATE = \"2024-11-15\"\n",
    "\n",
    "panel, treated_products, control_products = build_panel(\n",
    "    enriched_metrics, potential_outcomes, treatment_date=TREATMENT_DATE\n",
    ")\n",
    "\n",
    "# Pick showcase product: first treated product with above-median revenue\n",
    "avg_rev = panel.groupby(\"product_identifier\")[\"revenue\"].mean()\n",
    "treated_avg = avg_rev[avg_rev.index.isin(treated_products)]\n",
    "SHOWCASE_PRODUCT = sorted(treated_avg[treated_avg >= treated_avg.median()].index)[0]\n",
    "\n",
    "# Save panel for SC: showcase product + control products only (exclude other treated)\n",
    "sc_panel = panel[panel[\"product_identifier\"].isin([SHOWCASE_PRODUCT] + control_products)]\n",
    "sc_panel.to_csv(\"panel_data.csv\", index=False)\n",
    "\n",
    "# Ground truth for showcase product\n",
    "true_att = compute_ground_truth_att(panel, SHOWCASE_PRODUCT, TREATMENT_DATE)\n",
    "\n",
    "print(f\"Panel shape: {panel.shape}\")\n",
    "print(f\"Products: {panel['product_identifier'].nunique()}\")\n",
    "print(f\"  Treated: {len(treated_products)}\")\n",
    "print(f\"  Control: {len(control_products)}\")\n",
    "print(f\"Showcase product: {SHOWCASE_PRODUCT}\")\n",
    "print(f\"True ATT (showcase): ${true_att:,.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19",
   "metadata": {},
   "source": [
    "## 2. What Does the Naive Comparison Tell Us?\n",
    "\n",
    "As a baseline, we compute a simple **before-after difference** for the showcase product: compare its average daily revenue in the post-treatment period to its average in the pre-treatment period. We use the Impact Engine with an experiment configuration to run this as an OLS regression of revenue on a post-treatment indicator.\n",
    "\n",
    "This approach ignores the donor pool entirely. Any change in the treated product's revenue—whether caused by the campaign or by common time trends—is attributed to the treatment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {},
   "outputs": [],
   "source": [
    "! cat \"config_experiment.yaml\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Naive before-after comparison for the showcase product\n",
    "treated_data = panel[panel[\"product_identifier\"] == SHOWCASE_PRODUCT].copy()\n",
    "treated_data[\"post\"] = (treated_data[\"date\"] >= pd.Timestamp(TREATMENT_DATE)).astype(int)\n",
    "\n",
    "# Save for the Impact Engine pipeline\n",
    "treated_data.to_csv(\"treated_data.csv\", index=False)\n",
    "\n",
    "naive_job = evaluate_impact(\"config_experiment.yaml\", storage_url=\"./output/experiment\")\n",
    "naive_result = load_results(naive_job)\n",
    "naive_estimate = naive_result.impact_results[\"data\"][\"impact_estimates\"][\"params\"][\"post\"]\n",
    "\n",
    "print(\"Naive Before-After Comparison\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Pre-treatment mean:  ${treated_data[treated_data['post'] == 0]['revenue'].mean():,.2f}\")\n",
    "print(f\"Post-treatment mean: ${treated_data[treated_data['post'] == 1]['revenue'].mean():,.2f}\")\n",
    "print(f\"Naive estimate:      ${naive_estimate:,.2f}\")\n",
    "print(f\"\\nTrue ATT:            ${true_att:,.2f}\")\n",
    "print(f\"Bias:                ${naive_estimate - true_att:,.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22",
   "metadata": {},
   "source": [
    "### Why Does the Naive Estimate Fail?\n",
    "\n",
    "The before-after comparison confounds the treatment effect with **common time trends**. All products in the data experience a shared upward trend in revenue over the study period. The naive estimator attributes this trend entirely to the campaign, biasing the estimate upward.\n",
    "\n",
    "The synthetic control method solves this problem by constructing a counterfactual from the donor pool. Since the untreated products share the same time trends but were not affected by the campaign, their weighted combination provides a valid counterfactual that \"subtracts out\" the common trajectory."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23",
   "metadata": {},
   "source": [
    "## 3. Synthetic Control with the Impact Engine\n",
    "\n",
    "The [**Impact Engine**](https://github.com/eisenhauerIO/tools-impact-engine-measure) implements the synthetic control method from Part I. It uses `pysyncon` under the hood to find the optimal donor weights that minimize pre-treatment MSPE and estimates the average treatment effect on the treated (ATT) in the post-treatment period."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24",
   "metadata": {},
   "source": [
    "### Interface-to-Theory Mapping\n",
    "\n",
    "| YAML Config Field | Part I Concept |\n",
    "|-------------------|----------------|\n",
    "| `unit_column` | Unit identifier $j$ in the donor pool ($j = 2, \\ldots, J+1$) |\n",
    "| `time_column` | Time index $t$ in the panel |\n",
    "| `outcome_column` | Outcome $Y_{jt}$ for unit $j$ at time $t$ |\n",
    "| `treated_unit` | Treated unit $j=1$ receiving the intervention |\n",
    "| `treatment_time` | Intervention time $T_0$ splitting pre/post periods |\n",
    "| `optim_method` | Optimization for weight selection: $\\min \\|X_1 - X_0 W\\|$ |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25",
   "metadata": {},
   "outputs": [],
   "source": [
    "write_sc_config(SHOWCASE_PRODUCT, TREATMENT_DATE)\n",
    "! cat \"config_synthetic_control.yaml\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the synthetic control pipeline\n",
    "sc_job = evaluate_impact(\"config_synthetic_control.yaml\", storage_url=\"./output/synthetic_control\")\n",
    "sc_result = load_results(sc_job)\n",
    "\n",
    "sc_data = sc_result.impact_results[\"data\"]\n",
    "sc_att = sc_data[\"impact_estimates\"][\"att\"]\n",
    "sc_se = sc_data[\"impact_estimates\"][\"se\"]\n",
    "sc_mspe = sc_data[\"model_summary\"][\"mspe\"]\n",
    "\n",
    "print(\"Synthetic Control Results\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Estimated ATT:  ${sc_att:,.2f}  (SE: ${sc_se:,.2f})\")\n",
    "print(f\"True ATT:       ${true_att:,.2f}\")\n",
    "print(f\"Bias:           ${sc_att - true_att:,.2f}\")\n",
    "print(f\"\\nPre-treatment MSPE: {sc_mspe:,.4f}\")\n",
    "print(f\"Control units used: {sc_data['model_summary']['n_control_units']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27",
   "metadata": {},
   "source": [
    "## 4. Which Method Best Recovers the True Effect?\n",
    "\n",
    "We now compare the naive before-after estimate against the synthetic control estimate. Because we generated the data with known potential outcomes, we can directly measure each method's bias."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_method_comparison(\n",
    "    {\"Naive\\n(Before-After)\": naive_estimate, \"Synthetic\\nControl\": sc_att},\n",
    "    true_att,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary statistics table\n",
    "summary = pd.DataFrame(\n",
    "    {\n",
    "        \"Method\": [\"Naive (Before-After)\", \"Synthetic Control\"],\n",
    "        \"Estimate ($)\": [naive_estimate, sc_att],\n",
    "        \"Error ($)\": [naive_estimate - true_att, sc_att - true_att],\n",
    "        \"% Error\": [\n",
    "            (naive_estimate - true_att) / true_att * 100,\n",
    "            (sc_att - true_att) / true_att * 100,\n",
    "        ],\n",
    "    }\n",
    ")\n",
    "summary[\"Estimate ($)\"] = summary[\"Estimate ($)\"].map(lambda x: f\"${x:,.2f}\")\n",
    "summary[\"Error ($)\"] = summary[\"Error ($)\"].map(lambda x: f\"${x:,.2f}\")\n",
    "summary[\"% Error\"] = summary[\"% Error\"].map(lambda x: f\"{x:+.1f}%\")\n",
    "\n",
    "print(f\"True ATT: ${true_att:,.2f}\")\n",
    "print()\n",
    "summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30",
   "metadata": {},
   "source": [
    "## 5. Diagnostics\n",
    "\n",
    "Part I Section 6 outlined three practical requirements for credible synthetic control analysis: the donor pool must contain **plausible comparisons**, the **pre-treatment fit** must be tight, and the treated unit's outcome must lie within the **convex hull** of donor outcomes. We now evaluate each of these using four diagnostics: donor weights, the treated-vs-synthetic time series, the gap plot, and aggregate tracking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_weights(sc_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32",
   "metadata": {},
   "source": [
    "The weights reveal which donor units contribute to the synthetic control and how much. A sparse weight distribution—where only a few donors receive substantial weight—indicates that the method found a small set of products whose revenue trajectories closely resemble the showcase product's. This is preferable to diffuse weights spread across many donors, which would suggest no single combination fits well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33",
   "metadata": {},
   "outputs": [],
   "source": [
    "synthetic_ts = plot_treated_vs_synthetic(sc_panel, SHOWCASE_PRODUCT, sc_data, TREATMENT_DATE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34",
   "metadata": {},
   "source": [
    "The pre-treatment fit is the most important diagnostic. If the synthetic control closely tracks the treated product before the campaign, then the post-treatment divergence is credible evidence of a treatment effect. A tight pre-treatment fit confirms that the donor pool satisfies the convex hull condition from Part I Section 6—the showcase product's trajectory can be reproduced as a weighted average of untreated products."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_gap(sc_panel, SHOWCASE_PRODUCT, synthetic_ts, TREATMENT_DATE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36",
   "metadata": {},
   "source": [
    "The gap plot isolates the treatment effect over time. Pre-treatment, the gap should fluctuate around zero—confirming a good fit. Post-treatment, a sustained positive gap indicates the campaign increased revenue above what the synthetic control predicts. The shaded area represents the cumulative treatment effect that the synthetic control method recovers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37",
   "metadata": {},
   "source": [
    "### Aggregate Tracking\n",
    "\n",
    "The single-unit plots above show how synthetic control works for one product. But with 50 treated products and 450 controls, we can also examine whether treated and control groups tracked each other at the aggregate level before the campaign."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_average_fit(panel, treated_products, control_products, TREATMENT_DATE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39",
   "metadata": {},
   "source": [
    "## Additional resources\n",
    "\n",
    "- **Abadie, A. & Gardeazabal, J. (2003)**. [The economic costs of conflict: A case study of the Basque Country](https://doi.org/10.1257/000282803321455188). *American Economic Review*, 93(1), 113-132.\n",
    "\n",
    "- **Abadie, A., Diamond, A., & Hainmueller, J. (2010)**. [Synthetic control methods for comparative case studies: Estimating the effect of California's tobacco control program](https://doi.org/10.1198/jasa.2009.ap08746). *Journal of the American Statistical Association*, 105(490), 493-505.\n",
    "\n",
    "- **Abadie, A., Diamond, A., & Hainmueller, J. (2015)**. [Comparative politics and the synthetic control method](https://doi.org/10.1111/ajps.12116). *American Journal of Political Science*, 59(2), 495-510.\n",
    "\n",
    "- **Abadie, A. (2021)**. [Using synthetic controls: Feasibility, data requirements, and methodological aspects](https://doi.org/10.1257/jel.20191450). *Journal of Economic Literature*, 59(2), 391-425.\n",
    "\n",
    "- **Card, D. (1990)**. [The impact of the Mariel Boatlift on the Miami labor market](https://doi.org/10.2307/2523702). *Industrial and Labor Relations Review*, 43(2), 245-257.\n",
    "\n",
    "- **Cunningham, S. (2021)**. [*Causal Inference: The Mixtape*](https://mixtape.scunning.com/). Yale University Press. Chapter 10: Synthetic Control."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
