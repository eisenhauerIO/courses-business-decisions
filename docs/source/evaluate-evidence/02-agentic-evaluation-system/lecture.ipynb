{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Building an Agentic Evaluation System\n",
    "\n",
    "The `impact-engine-evaluate` package automates evidence assessment as the bridge between the MEASURE and ALLOCATE stages of the decision pipeline. This lecture develops both the principles and the engineering patterns that make automated evidence assessment trustworthy. The principles — failure modes, defensibility pillars, escalation, and the Assess vs. Improve discipline — apply to any agentic evaluation system. The design patterns — registry dispatch, prompt engineering as software, layered specialization, and structured output — show how these principles are instantiated in production code.\n",
    "\n",
    "In Part I we develop the principles for trustworthy automated assessment and then examine the design patterns that implement them. In Part II we read the source code of the `impact-engine-evaluate` package to see each pattern in practice."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part I: Principles and Design Patterns\n",
    "\n",
    "Agentic systems use LLMs as reasoning components within a structured software pipeline — not open-ended chat, but constrained evaluation with typed inputs and outputs. We develop the material in two layers. The first establishes the principles that make automated assessment trustworthy — what can go wrong and what must hold. The second examines the software design patterns that implement those principles in the `impact-engine-evaluate` package."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {},
   "source": [
    "## 1. Principles for Trustworthy Automated Assessment\n",
    "\n",
    "The diagnostic framework in Lecture 1 applies when a human analyst reviews a study. When we automate this review — using an LLM to assess measurement artifacts and assign a confidence score — four additional failure modes emerge that must be addressed by design.\n",
    "\n",
    "### Four Failure Modes of Automated Confidence Scoring\n",
    "\n",
    "| Failure Mode | What Goes Wrong | Example |\n",
    "|---|---|---|\n",
    "| **Ungroundedness** | The score is not traceable to any observable artifact | The system produces \"confidence: 0.73\" without citing any diagnostic |\n",
    "| **Incorrectness** | Artifacts are present but misread | An unbalanced covariate table is narrated as supportive |\n",
    "| **Opacity** | No audit trail — the score cannot be challenged or inspected | \"Why 0.73 and not 0.85?\" yields a plausible but unfalsifiable answer |\n",
    "| **Instability** | Same evidence described slightly differently yields a different score | Confidence becomes a function of prompt phrasing, not measurement quality |\n",
    "\n",
    "A confidence score that cannot be defended is worse than no confidence score at all. In enterprise settings where decisions are audited and revisited, these failure modes are fatal.\n",
    "\n",
    "### Four Pillars of Defensible Confidence\n",
    "\n",
    "Each pillar directly addresses one failure mode:\n",
    "\n",
    "| Pillar | Principle | Failure Mode Addressed | Mechanism |\n",
    "|---|---|---|---|\n",
    "| **Groundedness** | Every confidence claim traces to an observable statistical artifact | Ungroundedness | Common support checks, assumption tests, robustness diagnostics |\n",
    "| **Correctness** | The system interprets those artifacts accurately | Incorrectness | Evals on synthetic artifacts with known ground truth |\n",
    "| **Traceability** | Full audit trail from score to source data | Opacity | Per-dimension scores linked to specific diagnostics |\n",
    "| **Reproducibility** | Same pipeline + same data = same assessment | Instability | Fixed prompts, structured schemas, version-pinned backends |\n",
    "\n",
    "The pillars have a natural dependency order. Groundedness is the precondition: without observable artifacts, there is nothing to be correct about. Correctness builds on groundedness: the system must read the evidence accurately. Traceability makes correctness inspectable: when interpretations are wrong, the audit trail reveals where. Reproducibility ensures all three hold across runs, not just on a single evaluation.\n",
    "\n",
    "### The LLM as Narrator, Not Oracle\n",
    "\n",
    "The key architectural implication of these pillars is a precise division of labor. The LLM's role is to *contextualize* diagnostics that the measurement engine already produces — it does not generate confidence from its own internal probabilities or invent evidence. It is a narrator reading a score sheet, not an oracle pronouncing judgment.\n",
    "\n",
    "This bounds the LLM's contribution to interpretation within a constrained evidence set, which is what makes the output auditable. The correctness of that interpretation is then the empirical question that the evaluation framework in Lecture 3 assesses. The sections that follow examine how these four pillars are instantiated: first through the escalation patterns and the Assess vs. Improve discipline, then through the software design patterns — registry dispatch, prompt engineering, layered specialization, and structured output — that enforce the pillars in the `impact-engine-evaluate` codebase."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3",
   "metadata": {},
   "source": [
    "## 2. Evaluation Escalation: Judge, Jury, Reviewer, Debate\n",
    "\n",
    "**The problem**: A single-pass LLM evaluation may carry systematic biases that are invisible without a second perspective. High-stakes decisions require stronger guarantees than a base single-pass review provides. But adding complexity indiscriminately raises cost without commensurate benefit.\n",
    "\n",
    "**The pattern**: Four evaluation configurations form a complexity ladder. The decision to move up the ladder is always driven by evidence from the evaluation suite — not by default.\n",
    "\n",
    "| Pattern | Structure | What It Adds | When to Use |\n",
    "|---|---|---|---|\n",
    "| **Judge** | One LLM, one pass | Baseline | Default for all evaluations |\n",
    "| **Jury** | Multiple LLMs, parallel | Robustness through independence | Backend sensitivity is high on specific diagnostic types |\n",
    "| **Reviewer** | Two passes, sequential | Depth through self-correction | Subtle flaws are systematically missed |\n",
    "| **Debate** | Two LLMs, adversarial | Rigor through opposition | High-stakes estimates that need stress-testing |\n",
    "\n",
    "**Judge** is the right default. It is simple, fast, and fully covered by the four pillars developed above. Its limitation is that a single LLM may carry systematic biases — consistently lenient on robustness, consistently harsh on data quality — that are invisible without a second perspective.\n",
    "\n",
    "**Jury** runs multiple LLMs independently on the same artifact with the same prompt (the Panel of LLMs pattern). Agreement across backends reduces intra-model bias. Importantly, *disagreement* is a direct signal of rubric under-specification: the same diagnostic described the same way should not produce divergent scores. Promote to Jury when internal validity tests reveal high backend sensitivity.\n",
    "\n",
    "**Reviewer** adds a sequential critique pass: a second LLM evaluates whether the first pass's justification adequately addresses the diagnostic evidence and challenges gaps. The revised output is more thorough. Promote to Reviewer when external validity tests show the system misses subtle diagnostic features.\n",
    "\n",
    "**Debate** places two LLMs in opposing positions on the same artifact — one argues for high confidence, the other against — and a structured resolution produces the final score. This forces explicit engagement with the strongest counterargument. Reserve for high-stakes evaluations where a wrong score drives large allocation decisions.\n",
    "\n",
    "### Pattern Selection\n",
    "\n",
    "The patterns form a ladder; you climb it only when the eval suite tells you to:\n",
    "\n",
    "| Signal from Evaluation | Pattern Response |\n",
    "|---|---|\n",
    "| Scores are stable and correct | Stay at **Judge** |\n",
    "| Backend sensitivity is high | Move to **Jury** |\n",
    "| Subtle flaws are missed | Add **Reviewer** |\n",
    "| High-stakes edge cases need stress-testing | Deploy **Debate** selectively |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4",
   "metadata": {},
   "source": [
    "## 3. Assess vs. Improve\n",
    "\n",
    "**The problem**: When an evaluation system produces wrong answers, the instinct is to fix the prompts against the artifacts that revealed the problem. This overfits the fix to specific cases while potentially introducing new failures elsewhere — the same mistake made when tuning a model against its test set.\n",
    "\n",
    "**The pattern**: Measuring how the system performs (Assess mode) and acting on what you learn (Improve mode) must be strictly separated.\n",
    "\n",
    "| | Assess | Improve |\n",
    "|---|---|---|\n",
    "| **Purpose** | Measure current performance | Act on identified failure patterns |\n",
    "| **Activity** | Run internal and external validity test suites; produce a scorecard | Modify prompts, refine rubrics, tighten output schemas |\n",
    "| **Output** | Eval matrix: artifacts × configurations, pass/fail per cell | Updated prompt templates, rubrics, or schemas |\n",
    "| **Constraint** | **Read-only.** No changes to the system under test. | Validate fixes on **held-out** artifacts — never the ones that revealed the problem. |\n",
    "\n",
    "### Internal vs. External Validity of the Evaluation System\n",
    "\n",
    "The Assess mode applies the same internal/external validity distinction introduced in Lecture 1 — now to the automated system itself.\n",
    "\n",
    "**Internal validity** tests whether the system behaves coherently under variation in its own components, without requiring ground truth:\n",
    "- *Run-to-run stability*: same artifact + same config → same score\n",
    "- *Prompt sensitivity*: semantically equivalent prompts → consistent scores\n",
    "- *Backend sensitivity*: same artifact across different LLM backends → low divergence\n",
    "- *Score distribution*: does the system use the full [0, 1] range, or cluster around a narrow band?\n",
    "\n",
    "**External validity** tests whether the system gets the right answer, using synthetic artifacts with known properties:\n",
    "- *Known-flaw detection*: deliberately flawed diagnostics (poor balance, high attrition) must be flagged\n",
    "- *Known-clean scoring*: a well-powered RCT with perfect diagnostics must score highly without invented concerns\n",
    "- *Severity calibration*: scores must decrease monotonically as flaw severity increases\n",
    "\n",
    "Internal validity is the precondition for external validity: a system that gives different answers on every run cannot be meaningfully tested against ground truth.\n",
    "\n",
    "### Failure Pattern → Intervention\n",
    "\n",
    "When Assess mode reveals a pattern, Improve mode responds with a targeted fix:\n",
    "\n",
    "| Failure Pattern | Root Cause | Intervention |\n",
    "|---|---|---|\n",
    "| All backends misread a diagnostic type | Rubric under-specified for that artifact | Add explicit scoring criteria to the dimension prompt |\n",
    "| One backend consistently misses an issue | Model lacks required capability | Simplify prompt or add backend-specific variant |\n",
    "| Scores shift across equivalent prompts | Evaluation task not well-defined | Tighten rubric to leave less interpretive room |\n",
    "\n",
    "Every intervention is validated on held-out artifacts — artifacts that were not part of the diagnosis. The Assess–Improve cycle governs how the evaluation framework matures over time without access to downstream outcome data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5",
   "metadata": {},
   "source": [
    "## 4. Registry + Dispatch\n",
    "\n",
    "**The problem**: A system needs to route different inputs to different handlers — but hardcoding that routing creates fragile, hard-to-extend code.\n",
    "\n",
    "**The pattern**: Each handler registers itself with a central registry, keyed by an identifier. At runtime, the system reads the identifier from the input (e.g., from a configuration file or manifest) and dispatches to the correct handler automatically.\n",
    "\n",
    "```python\n",
    "@MethodReviewerRegistry.register(\"experiment\")\n",
    "class ExperimentReviewer(MethodReviewer):\n",
    "    confidence_range = (0.85, 1.0)\n",
    "    ...\n",
    "```\n",
    "\n",
    "The registry pattern separates *what handlers exist* from *how they are selected*. Adding support for a new methodology means implementing a new class and registering it — the dispatch logic remains unchanged. The same pattern works for LLM backends: a `BackendRegistry` routes to Anthropic, OpenAI, or LiteLLM based on configuration."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6",
   "metadata": {},
   "source": [
    "## 5. Prompt Engineering as Software\n",
    "\n",
    "**The problem**: Prompts written inline as strings become unmaintainable — they mix concerns, lack versioning, and cannot be reviewed like code.\n",
    "\n",
    "**The pattern**: Prompts are versioned artifacts stored in structured files (YAML, JSON) with explicit metadata. Templates use a dedicated engine (e.g., Jinja2) to inject context at runtime, separating *what to evaluate* from *how to evaluate*.\n",
    "\n",
    "```yaml\n",
    "name: experiment_review\n",
    "version: \"1.0\"\n",
    "dimensions:\n",
    "  - randomization_integrity\n",
    "  - statistical_inference\n",
    "  ...\n",
    "\n",
    "system: |\n",
    "  You are a methodological reviewer...\n",
    "  {{ knowledge_context }}\n",
    "\n",
    "user: |\n",
    "  Review the following artifact:\n",
    "  {{ artifact }}\n",
    "```\n",
    "\n",
    "**Knowledge injection** is a key element: domain expertise files — markdown documents encoding design principles, common pitfalls, and diagnostic standards — are loaded from disk and injected into the prompt. This grounds the LLM's assessment in documented domain knowledge rather than relying solely on its training data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7",
   "metadata": {},
   "source": [
    "## 6. Layered Specialization\n",
    "\n",
    "**The problem**: Multiple reviewers share common orchestration logic but differ in method-specific details. Duplicating the orchestration code across reviewers creates maintenance burden.\n",
    "\n",
    "**The pattern**: An abstract base class defines the interface — the contract that all reviewers must satisfy. Concrete subclasses supply only the method-specific details (prompt templates, knowledge files, confidence ranges). The orchestration layer operates against the interface, unaware of which concrete class it is using.\n",
    "\n",
    "| Layer | Responsibility |\n",
    "|-------|----------------|\n",
    "| `MethodReviewer` (ABC) | Interface: `load_artifact()`, `prompt_template_dir()`, `knowledge_content_dir()`, `confidence_range` |\n",
    "| `ExperimentReviewer` | Experiment-specific: prompt directory, knowledge directory, `(0.85, 1.0)` range |\n",
    "\n",
    "Adding a new methodology (e.g., difference-in-differences) requires implementing one class with the method-specific details — the `evaluate_confidence` function and `ReviewEngine` orchestration logic remain unchanged."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8",
   "metadata": {},
   "source": [
    "## 7. Structured Output\n",
    "\n",
    "**The problem**: LLMs produce free-form text, but downstream systems need typed, machine-readable data. Free-form output cannot be consumed reliably by code.\n",
    "\n",
    "**The pattern**: The system constrains the LLM's output format in the prompt, then parses the response into typed objects. Multiple fallback strategies handle format deviations:\n",
    "\n",
    "1. The prompt specifies an exact format: `DIMENSION: / SCORE: / JUSTIFICATION:` blocks\n",
    "2. A regex parser extracts per-dimension scores and justifications\n",
    "3. A JSON fallback handles alternative response formats\n",
    "4. All scores are clamped to `[0.0, 1.0]` and assembled into a typed `ReviewResult`\n",
    "\n",
    "This **constrain → parse → validate** cycle is fundamental to agentic systems. The LLM provides reasoning and judgment; the surrounding code ensures the output is reliable and machine-readable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part II: Implementation\n",
    "\n",
    "We now read the source code of the evaluate tool to see each pattern in practice. The goal is not to memorize the implementation, but to recognize the design choices that each pattern reflects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "import inspect\n",
    "\n",
    "from impact_engine_evaluate import evaluate_confidence\n",
    "from impact_engine_evaluate.api import EvaluationRouter\n",
    "from impact_engine_evaluate.review.methods.base import MethodReviewer, MethodReviewerRegistry\n",
    "from impact_engine_evaluate.review.methods.experiment.reviewer import ExperimentReviewer\n",
    "from impact_engine_evaluate.review.models import ReviewDimension, ReviewResult\n",
    "from IPython.display import Code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11",
   "metadata": {},
   "source": [
    "## Registry + Dispatch in Practice\n",
    "\n",
    "The `MethodReviewerRegistry` class implements the registry pattern. It maintains a dictionary mapping method names to reviewer classes, and exposes `register()` as a class decorator:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "Code(inspect.getsource(MethodReviewerRegistry), language=\"python\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13",
   "metadata": {},
   "source": [
    "The registry is populated when reviewer modules are imported. We can inspect which methods are currently registered and what confidence range each carries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, (lo, hi) in MethodReviewerRegistry.confidence_map().items():\n",
    "    print(f\"  {name}: confidence_range = ({lo:.2f}, {hi:.2f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15",
   "metadata": {},
   "source": [
    "The `ExperimentReviewer` shows how registration works in practice — a single decorator line registers the class with the registry:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "Code(inspect.getsource(ExperimentReviewer), language=\"python\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17",
   "metadata": {},
   "source": [
    "## Prompt Engineering as Software in Practice\n",
    "\n",
    "The prompt template for experiment review is a YAML file with explicit metadata, structured dimensions, and Jinja2 template variables. We read it directly from the package:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "reviewer = ExperimentReviewer()\n",
    "template_path = reviewer.prompt_template_dir() / \"experiment_review.yaml\"\n",
    "print(template_path.read_text())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19",
   "metadata": {},
   "source": [
    "Knowledge files ground the LLM's assessment in documented domain expertise. Each file encodes a specific aspect of experimental design:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {},
   "outputs": [],
   "source": [
    "knowledge_dir = reviewer.knowledge_content_dir()\n",
    "for path in sorted(knowledge_dir.iterdir()):\n",
    "    if path.suffix in (\".md\", \".txt\"):\n",
    "        lines = path.read_text().splitlines()\n",
    "        print(f\"--- {path.name} ({len(lines)} lines) ---\")\n",
    "        print(\"\\n\".join(lines[:8]))\n",
    "        print(\"...\")\n",
    "        print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21",
   "metadata": {},
   "source": [
    "## Layered Specialization in Practice\n",
    "\n",
    "The `MethodReviewer` abstract base class defines the interface that all reviewers must satisfy. Concrete subclasses provide method-specific implementations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22",
   "metadata": {},
   "outputs": [],
   "source": [
    "Code(inspect.getsource(MethodReviewer), language=\"python\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23",
   "metadata": {},
   "source": [
    "## Structured Output in Practice\n",
    "\n",
    "The `ReviewResult` and `ReviewDimension` dataclasses define the typed output structure that the parsing logic assembles from the LLM's free-form response:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24",
   "metadata": {},
   "outputs": [],
   "source": [
    "Code(inspect.getsource(ReviewResult), language=\"python\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25",
   "metadata": {},
   "outputs": [],
   "source": [
    "Code(inspect.getsource(ReviewDimension), language=\"python\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26",
   "metadata": {},
   "source": [
    "## Connecting the Patterns: evaluate_confidence\n",
    "\n",
    "The `evaluate_confidence` function shows how all four patterns compose into the full pipeline:\n",
    "\n",
    "1. **Registry dispatch**: `EvaluationRouter.route()` calls `MethodReviewerRegistry.create(manifest.model_type)` to select the reviewer\n",
    "2. **Strategy dispatch**: branches on `\"score\"` vs `\"review\"` — only the confidence source differs\n",
    "3. **Prompt templates + knowledge injection**: handled inside `review()`, using the reviewer's template and knowledge directories\n",
    "4. **Structured output**: `review()` returns a typed `ReviewResult`; the function returns a typed `EvaluateResult` with guaranteed fields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27",
   "metadata": {},
   "outputs": [],
   "source": [
    "Code(inspect.getsource(evaluate_confidence), language=\"python\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28",
   "metadata": {},
   "source": [
    "## Escalation and Assess vs. Improve in Practice\n",
    "\n",
    "The `evaluate_confidence` function currently implements the **Judge** pattern by default: a single LLM pass constrained to structured output. The `evaluate_strategy` field in the manifest selects between deterministic scoring (`\"score\"`) and the full agentic reviewer (`\"review\"`), but both branches use a single-pass Judge.\n",
    "\n",
    "The escalation ladder and the Assess vs. Improve discipline are implemented *around* this function, not within it:\n",
    "\n",
    "- **Jury** would call `evaluate_confidence` with multiple different backend configurations and aggregate or compare their `ReviewResult` outputs.\n",
    "- **Reviewer** would call `evaluate_confidence` once, then pass the `ReviewResult` back to a critique pass that checks whether the justifications adequately address each diagnostic.\n",
    "- **Debate** would call `evaluate_confidence` twice with adversarial system prompts, then run a resolution step.\n",
    "\n",
    "The `confidence_map()` output seen above is relevant here: the confidence ranges reflect the *hierarchy of evidence* (a structural prior about methodology quality), while the escalation pattern choice reflects *how thoroughly to interrogate a specific estimate*. These are orthogonal decisions — you can run a Debate-level review on a quasi-experimental study, or stay at Judge for a well-powered RCT.\n",
    "\n",
    "The Assess vs. Improve discipline likewise operates around `evaluate_confidence`: Assess mode calls it repeatedly on synthetic artifacts with known properties (clean vs. flawed), records pass/fail per dimension, and produces a scorecard. Improve mode then modifies the prompt templates and rubrics based on patterns in that scorecard — and validates every change on held-out artifacts that were not part of the diagnosis. Lecture 3 demonstrates the Assess mode concretely: we run known-clean and known-flaw artifacts through the reviewer and verify that the system discriminates appropriately."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29",
   "metadata": {},
   "source": [
    "## Additional resources\n",
    "\n",
    "- [Anthropic: Building Effective Agents](https://www.anthropic.com/engineering/building-effective-agents) — Design patterns for LLM-powered systems\n",
    "- [Anthropic: Claude Agent SDK](https://docs.anthropic.com/en/docs/agents-and-tools/claude-agent-sdk/overview) — SDK for building agents with Claude"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
