{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Building an Agentic Evaluation System\n",
    "\n",
    "The `impact-engine-evaluate` package automates evidence assessment as the bridge between the MEASURE and ALLOCATE stages of the decision pipeline. This lecture examines how it is built — not how to use it, but what design patterns make it work. Understanding these patterns matters because the same patterns appear in any production agentic system: registry dispatch, versioned prompt templates, layered specialization, and structured output parsing.\n",
    "\n",
    "In Part I we develop the conceptual foundation for each pattern. In Part II we read the source code of the evaluate tool to see each pattern in practice."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part I: Design Patterns for Agentic Systems\n",
    "\n",
    "Agentic systems use LLMs as reasoning components within a structured software pipeline — not open-ended chat, but constrained evaluation with typed inputs and outputs. Four design patterns recur across well-engineered agentic systems."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {},
   "source": [
    "## 1. Registry + Dispatch\n",
    "\n",
    "**The problem**: A system needs to route different inputs to different handlers — but hardcoding that routing creates fragile, hard-to-extend code.\n",
    "\n",
    "**The pattern**: Each handler registers itself with a central registry, keyed by an identifier. At runtime, the system reads the identifier from the input (e.g., from a configuration file or manifest) and dispatches to the correct handler automatically.\n",
    "\n",
    "```python\n",
    "@MethodReviewerRegistry.register(\"experiment\")\n",
    "class ExperimentReviewer(MethodReviewer):\n",
    "    confidence_range = (0.85, 1.0)\n",
    "    ...\n",
    "```\n",
    "\n",
    "The registry pattern separates *what handlers exist* from *how they are selected*. Adding support for a new methodology means implementing a new class and registering it — the dispatch logic remains unchanged. The same pattern works for LLM backends: a `BackendRegistry` routes to Anthropic, OpenAI, or LiteLLM based on configuration."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3",
   "metadata": {},
   "source": [
    "## 2. Prompt Engineering as Software\n",
    "\n",
    "**The problem**: Prompts written inline as strings become unmaintainable — they mix concerns, lack versioning, and cannot be reviewed like code.\n",
    "\n",
    "**The pattern**: Prompts are versioned artifacts stored in structured files (YAML, JSON) with explicit metadata. Templates use a dedicated engine (e.g., Jinja2) to inject context at runtime, separating *what to evaluate* from *how to evaluate*.\n",
    "\n",
    "```yaml\n",
    "name: experiment_review\n",
    "version: \"1.0\"\n",
    "dimensions:\n",
    "  - randomization_integrity\n",
    "  - statistical_inference\n",
    "  ...\n",
    "\n",
    "system: |\n",
    "  You are a methodological reviewer...\n",
    "  {{ knowledge_context }}\n",
    "\n",
    "user: |\n",
    "  Review the following artifact:\n",
    "  {{ artifact }}\n",
    "```\n",
    "\n",
    "**Knowledge injection** is a key element: domain expertise files — markdown documents encoding design principles, common pitfalls, and diagnostic standards — are loaded from disk and injected into the prompt. This grounds the LLM's assessment in documented domain knowledge rather than relying solely on its training data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4",
   "metadata": {},
   "source": [
    "## 3. Layered Specialization\n",
    "\n",
    "**The problem**: Multiple reviewers share common orchestration logic but differ in method-specific details. Duplicating the orchestration code across reviewers creates maintenance burden.\n",
    "\n",
    "**The pattern**: An abstract base class defines the interface — the contract that all reviewers must satisfy. Concrete subclasses supply only the method-specific details (prompt templates, knowledge files, confidence ranges). The orchestration layer operates against the interface, unaware of which concrete class it is using.\n",
    "\n",
    "| Layer | Responsibility |\n",
    "|-------|----------------|\n",
    "| `MethodReviewer` (ABC) | Interface: `load_artifact()`, `prompt_template_dir()`, `knowledge_content_dir()`, `confidence_range` |\n",
    "| `ExperimentReviewer` | Experiment-specific: prompt directory, knowledge directory, `(0.85, 1.0)` range |\n",
    "\n",
    "Adding a new methodology (e.g., difference-in-differences) requires implementing one class with the method-specific details — the `Evaluate` and `ReviewEngine` orchestration logic remains unchanged."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5",
   "metadata": {},
   "source": [
    "## 4. Structured Output\n",
    "\n",
    "**The problem**: LLMs produce free-form text, but downstream systems need typed, machine-readable data. Free-form output cannot be consumed reliably by code.\n",
    "\n",
    "**The pattern**: The system constrains the LLM's output format in the prompt, then parses the response into typed objects. Multiple fallback strategies handle format deviations:\n",
    "\n",
    "1. The prompt specifies an exact format: `DIMENSION: / SCORE: / JUSTIFICATION:` blocks\n",
    "2. A regex parser extracts per-dimension scores and justifications\n",
    "3. A JSON fallback handles alternative response formats\n",
    "4. All scores are clamped to `[0.0, 1.0]` and assembled into a typed `ReviewResult`\n",
    "\n",
    "This **constrain → parse → validate** cycle is fundamental to agentic systems. The LLM provides reasoning and judgment; the surrounding code ensures the output is reliable and machine-readable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part II: Implementation\n",
    "\n",
    "We now read the source code of the evaluate tool to see each pattern in practice. The goal is not to memorize the implementation, but to recognize the design choices that each pattern reflects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import inspect\n",
    "\n",
    "from impact_engine_evaluate import Evaluate\n",
    "from impact_engine_evaluate.review.methods.base import MethodReviewer, MethodReviewerRegistry\n",
    "from impact_engine_evaluate.review.methods.experiment.reviewer import ExperimentReviewer\n",
    "from impact_engine_evaluate.review.models import ReviewDimension, ReviewResult\n",
    "from IPython.display import Code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8",
   "metadata": {},
   "source": [
    "## 1. Registry + Dispatch in Practice\n",
    "\n",
    "The `MethodReviewerRegistry` class implements the registry pattern. It maintains a dictionary mapping method names to reviewer classes, and exposes `register()` as a class decorator:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "Code(inspect.getsource(MethodReviewerRegistry), language=\"python\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10",
   "metadata": {},
   "source": [
    "The registry is populated when reviewer modules are imported. We can inspect which methods are currently registered and what confidence range each carries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, (lo, hi) in MethodReviewerRegistry.confidence_map().items():\n",
    "    print(f\"  {name}: confidence_range = ({lo:.2f}, {hi:.2f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12",
   "metadata": {},
   "source": [
    "The `ExperimentReviewer` shows how registration works in practice — a single decorator line registers the class with the registry:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "Code(inspect.getsource(ExperimentReviewer), language=\"python\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14",
   "metadata": {},
   "source": [
    "## 2. Prompt Engineering as Software\n",
    "\n",
    "The prompt template for experiment review is a YAML file with explicit metadata, structured dimensions, and Jinja2 template variables. We read it directly from the package:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "reviewer = ExperimentReviewer()\n",
    "template_path = reviewer.prompt_template_dir() / \"experiment_review.yaml\"\n",
    "print(template_path.read_text())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16",
   "metadata": {},
   "source": [
    "Knowledge files ground the LLM's assessment in documented domain expertise. Each file encodes a specific aspect of experimental design:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "knowledge_dir = reviewer.knowledge_content_dir()\n",
    "for path in sorted(knowledge_dir.iterdir()):\n",
    "    if path.suffix in (\".md\", \".txt\"):\n",
    "        lines = path.read_text().splitlines()\n",
    "        print(f\"--- {path.name} ({len(lines)} lines) ---\")\n",
    "        print(\"\\n\".join(lines[:8]))\n",
    "        print(\"...\")\n",
    "        print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18",
   "metadata": {},
   "source": [
    "## 3. Layered Specialization\n",
    "\n",
    "The `MethodReviewer` abstract base class defines the interface that all reviewers must satisfy. Concrete subclasses provide method-specific implementations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": [
    "Code(inspect.getsource(MethodReviewer), language=\"python\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20",
   "metadata": {},
   "source": [
    "## 4. Structured Output\n",
    "\n",
    "The `ReviewResult` and `ReviewDimension` dataclasses define the typed output structure that the parsing logic assembles from the LLM's free-form response:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {},
   "outputs": [],
   "source": [
    "Code(inspect.getsource(ReviewResult), language=\"python\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22",
   "metadata": {},
   "outputs": [],
   "source": [
    "Code(inspect.getsource(ReviewDimension), language=\"python\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23",
   "metadata": {},
   "source": [
    "## Connecting the Patterns: Evaluate.execute\n",
    "\n",
    "The `Evaluate.execute` method shows how all four patterns compose into the full pipeline:\n",
    "\n",
    "1. **Registry dispatch**: `MethodReviewerRegistry.create(manifest.model_type)` selects the reviewer\n",
    "2. **Strategy dispatch**: branches on `\"score\"` vs `\"review\"` — only the confidence source differs\n",
    "3. **Prompt templates + knowledge injection**: handled inside `review()`, using the reviewer's template and knowledge directories\n",
    "4. **Structured output**: `review()` returns a typed `ReviewResult`; the method returns `asdict(result)` — a typed dict with guaranteed keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24",
   "metadata": {},
   "outputs": [],
   "source": [
    "Code(inspect.getsource(Evaluate), language=\"python\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25",
   "metadata": {},
   "source": [
    "## Additional Resources\n",
    "\n",
    "- [Anthropic: Building Effective Agents](https://www.anthropic.com/engineering/building-effective-agents) — Design patterns for LLM-powered systems\n",
    "- [Anthropic: Claude Agent SDK](https://docs.anthropic.com/en/docs/agents-and-tools/claude-agent-sdk/overview) — SDK for building agents with Claude"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
