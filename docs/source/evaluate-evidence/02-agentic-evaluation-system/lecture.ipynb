{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Building an Agentic Evaluation System\n",
    "\n",
    "The `impact-engine-evaluate` package automates evidence assessment as the bridge between the MEASURE and ALLOCATE stages of the decision pipeline. This lecture examines how it is built — not how to use it, but what design patterns make it work. Understanding these patterns matters because the same patterns appear in any production agentic system: registry dispatch, versioned prompt templates, layered specialization, structured output parsing, evaluation escalation, and the discipline of separating measurement from improvement.\n",
    "\n",
    "In Part I we develop the conceptual foundation for each pattern. In Part II we read the source code of the evaluate tool to see each pattern in practice."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part I: Design Patterns for Agentic Systems\n",
    "\n",
    "Agentic systems use LLMs as reasoning components within a structured software pipeline — not open-ended chat, but constrained evaluation with typed inputs and outputs. Six design patterns recur across well-engineered agentic systems."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {},
   "source": [
    "## 1. Registry + Dispatch\n",
    "\n",
    "**The problem**: A system needs to route different inputs to different handlers — but hardcoding that routing creates fragile, hard-to-extend code.\n",
    "\n",
    "**The pattern**: Each handler registers itself with a central registry, keyed by an identifier. At runtime, the system reads the identifier from the input (e.g., from a configuration file or manifest) and dispatches to the correct handler automatically.\n",
    "\n",
    "```python\n",
    "@MethodReviewerRegistry.register(\"experiment\")\n",
    "class ExperimentReviewer(MethodReviewer):\n",
    "    confidence_range = (0.85, 1.0)\n",
    "    ...\n",
    "```\n",
    "\n",
    "The registry pattern separates *what handlers exist* from *how they are selected*. Adding support for a new methodology means implementing a new class and registering it — the dispatch logic remains unchanged. The same pattern works for LLM backends: a `BackendRegistry` routes to Anthropic, OpenAI, or LiteLLM based on configuration."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3",
   "metadata": {},
   "source": [
    "## 2. Prompt Engineering as Software\n",
    "\n",
    "**The problem**: Prompts written inline as strings become unmaintainable — they mix concerns, lack versioning, and cannot be reviewed like code.\n",
    "\n",
    "**The pattern**: Prompts are versioned artifacts stored in structured files (YAML, JSON) with explicit metadata. Templates use a dedicated engine (e.g., Jinja2) to inject context at runtime, separating *what to evaluate* from *how to evaluate*.\n",
    "\n",
    "```yaml\n",
    "name: experiment_review\n",
    "version: \"1.0\"\n",
    "dimensions:\n",
    "  - randomization_integrity\n",
    "  - statistical_inference\n",
    "  ...\n",
    "\n",
    "system: |\n",
    "  You are a methodological reviewer...\n",
    "  {{ knowledge_context }}\n",
    "\n",
    "user: |\n",
    "  Review the following artifact:\n",
    "  {{ artifact }}\n",
    "```\n",
    "\n",
    "**Knowledge injection** is a key element: domain expertise files — markdown documents encoding design principles, common pitfalls, and diagnostic standards — are loaded from disk and injected into the prompt. This grounds the LLM's assessment in documented domain knowledge rather than relying solely on its training data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4",
   "metadata": {},
   "source": [
    "## 3. Layered Specialization\n",
    "\n",
    "**The problem**: Multiple reviewers share common orchestration logic but differ in method-specific details. Duplicating the orchestration code across reviewers creates maintenance burden.\n",
    "\n",
    "**The pattern**: An abstract base class defines the interface — the contract that all reviewers must satisfy. Concrete subclasses supply only the method-specific details (prompt templates, knowledge files, confidence ranges). The orchestration layer operates against the interface, unaware of which concrete class it is using.\n",
    "\n",
    "| Layer | Responsibility |\n",
    "|-------|----------------|\n",
    "| `MethodReviewer` (ABC) | Interface: `load_artifact()`, `prompt_template_dir()`, `knowledge_content_dir()`, `confidence_range` |\n",
    "| `ExperimentReviewer` | Experiment-specific: prompt directory, knowledge directory, `(0.85, 1.0)` range |\n",
    "\n",
    "Adding a new methodology (e.g., difference-in-differences) requires implementing one class with the method-specific details — the `Evaluate` and `ReviewEngine` orchestration logic remains unchanged."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5",
   "metadata": {},
   "source": [
    "## 4. Structured Output\n",
    "\n",
    "**The problem**: LLMs produce free-form text, but downstream systems need typed, machine-readable data. Free-form output cannot be consumed reliably by code.\n",
    "\n",
    "**The pattern**: The system constrains the LLM's output format in the prompt, then parses the response into typed objects. Multiple fallback strategies handle format deviations:\n",
    "\n",
    "1. The prompt specifies an exact format: `DIMENSION: / SCORE: / JUSTIFICATION:` blocks\n",
    "2. A regex parser extracts per-dimension scores and justifications\n",
    "3. A JSON fallback handles alternative response formats\n",
    "4. All scores are clamped to `[0.0, 1.0]` and assembled into a typed `ReviewResult`\n",
    "\n",
    "This **constrain → parse → validate** cycle is fundamental to agentic systems. The LLM provides reasoning and judgment; the surrounding code ensures the output is reliable and machine-readable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6",
   "metadata": {},
   "source": [
    "## 5. Evaluation Escalation: Judge, Jury, Reviewer, Debate\n",
    "\n",
    "**The problem**: A single-pass LLM evaluation may carry systematic biases that are invisible without a second perspective. High-stakes decisions require stronger guarantees than a base single-pass review provides. But adding complexity indiscriminately raises cost without commensurate benefit.\n",
    "\n",
    "**The pattern**: Four evaluation configurations form a complexity ladder. The decision to move up the ladder is always driven by evidence from the evaluation suite — not by default.\n",
    "\n",
    "| Pattern | Structure | What It Adds | When to Use |\n",
    "|---|---|---|---|\n",
    "| **Judge** | One LLM, one pass | Baseline | Default for all evaluations |\n",
    "| **Jury** | Multiple LLMs, parallel | Robustness through independence | Backend sensitivity is high on specific diagnostic types |\n",
    "| **Reviewer** | Two passes, sequential | Depth through self-correction | Subtle flaws are systematically missed |\n",
    "| **Debate** | Two LLMs, adversarial | Rigor through opposition | High-stakes estimates that need stress-testing |\n",
    "\n",
    "**Judge** is the right default. It is simple, fast, and fully covered by the four pillars from Lecture 1. Its limitation is that a single LLM may carry systematic biases — consistently lenient on robustness, consistently harsh on data quality — that are invisible without a second perspective.\n",
    "\n",
    "**Jury** runs multiple LLMs independently on the same artifact with the same prompt (the Panel of LLMs pattern). Agreement across backends reduces intra-model bias. Importantly, *disagreement* is a direct signal of rubric under-specification: the same diagnostic described the same way should not produce divergent scores. Promote to Jury when internal validity tests reveal high backend sensitivity.\n",
    "\n",
    "**Reviewer** adds a sequential critique pass: a second LLM evaluates whether the first pass's justification adequately addresses the diagnostic evidence and challenges gaps. The revised output is more thorough. Promote to Reviewer when external validity tests show the system misses subtle diagnostic features.\n",
    "\n",
    "**Debate** places two LLMs in opposing positions on the same artifact — one argues for high confidence, the other against — and a structured resolution produces the final score. This forces explicit engagement with the strongest counterargument. Reserve for high-stakes evaluations where a wrong score drives large allocation decisions.\n",
    "\n",
    "### Pattern Selection\n",
    "\n",
    "The patterns form a ladder; you climb it only when the eval suite tells you to:\n",
    "\n",
    "| Signal from Evaluation | Pattern Response |\n",
    "|---|---|\n",
    "| Scores are stable and correct | Stay at **Judge** |\n",
    "| Backend sensitivity is high | Move to **Jury** |\n",
    "| Subtle flaws are missed | Add **Reviewer** |\n",
    "| High-stakes edge cases need stress-testing | Deploy **Debate** selectively |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7",
   "metadata": {},
   "source": [
    "## 6. Assess vs. Improve\n",
    "\n",
    "**The problem**: When an evaluation system produces wrong answers, the instinct is to fix the prompts against the artifacts that revealed the problem. This overfits the fix to specific cases while potentially introducing new failures elsewhere — the same mistake made when tuning a model against its test set.\n",
    "\n",
    "**The pattern**: Measuring how the system performs (Assess mode) and acting on what you learn (Improve mode) must be strictly separated.\n",
    "\n",
    "| | Assess | Improve |\n",
    "|---|---|---|\n",
    "| **Purpose** | Measure current performance | Act on identified failure patterns |\n",
    "| **Activity** | Run internal and external validity test suites; produce a scorecard | Modify prompts, refine rubrics, tighten output schemas |\n",
    "| **Output** | Eval matrix: artifacts × configurations, pass/fail per cell | Updated prompt templates, rubrics, or schemas |\n",
    "| **Constraint** | **Read-only.** No changes to the system under test. | Validate fixes on **held-out** artifacts — never the ones that revealed the problem. |\n",
    "\n",
    "### Internal vs. External Validity of the Evaluation System\n",
    "\n",
    "The Assess mode applies the same internal/external validity distinction introduced in Lecture 1 — now to the automated system itself.\n",
    "\n",
    "**Internal validity** tests whether the system behaves coherently under variation in its own components, without requiring ground truth:\n",
    "- *Run-to-run stability*: same artifact + same config → same score\n",
    "- *Prompt sensitivity*: semantically equivalent prompts → consistent scores\n",
    "- *Backend sensitivity*: same artifact across different LLM backends → low divergence\n",
    "- *Score distribution*: does the system use the full [0, 1] range, or cluster around a narrow band?\n",
    "\n",
    "**External validity** tests whether the system gets the right answer, using synthetic artifacts with known properties:\n",
    "- *Known-flaw detection*: deliberately flawed diagnostics (poor balance, high attrition) must be flagged\n",
    "- *Known-clean scoring*: a well-powered RCT with perfect diagnostics must score highly without invented concerns\n",
    "- *Severity calibration*: scores must decrease monotonically as flaw severity increases\n",
    "\n",
    "Internal validity is the precondition for external validity: a system that gives different answers on every run cannot be meaningfully tested against ground truth.\n",
    "\n",
    "### Failure Pattern → Intervention\n",
    "\n",
    "When Assess mode reveals a pattern, Improve mode responds with a targeted fix:\n",
    "\n",
    "| Failure Pattern | Root Cause | Intervention |\n",
    "|---|---|---|\n",
    "| All backends misread a diagnostic type | Rubric under-specified for that artifact | Add explicit scoring criteria to the dimension prompt |\n",
    "| One backend consistently misses an issue | Model lacks required capability | Simplify prompt or add backend-specific variant |\n",
    "| Scores shift across equivalent prompts | Evaluation task not well-defined | Tighten rubric to leave less interpretive room |\n",
    "\n",
    "Every intervention is validated on held-out artifacts — artifacts that were not part of the diagnosis. The Assess–Improve cycle governs how the evaluation framework matures over time without access to downstream outcome data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part II: Implementation\n",
    "\n",
    "We now read the source code of the evaluate tool to see each pattern in practice. The goal is not to memorize the implementation, but to recognize the design choices that each pattern reflects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import inspect\n",
    "\n",
    "from impact_engine_evaluate import Evaluate\n",
    "from impact_engine_evaluate.review.methods.base import MethodReviewer, MethodReviewerRegistry\n",
    "from impact_engine_evaluate.review.methods.experiment.reviewer import ExperimentReviewer\n",
    "from impact_engine_evaluate.review.models import ReviewDimension, ReviewResult\n",
    "from IPython.display import Code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10",
   "metadata": {},
   "source": [
    "## Registry + Dispatch in Practice\n",
    "\n",
    "The `MethodReviewerRegistry` class implements the registry pattern. It maintains a dictionary mapping method names to reviewer classes, and exposes `register()` as a class decorator:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "Code(inspect.getsource(MethodReviewerRegistry), language=\"python\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12",
   "metadata": {},
   "source": [
    "The registry is populated when reviewer modules are imported. We can inspect which methods are currently registered and what confidence range each carries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, (lo, hi) in MethodReviewerRegistry.confidence_map().items():\n",
    "    print(f\"  {name}: confidence_range = ({lo:.2f}, {hi:.2f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14",
   "metadata": {},
   "source": [
    "The `ExperimentReviewer` shows how registration works in practice — a single decorator line registers the class with the registry:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "Code(inspect.getsource(ExperimentReviewer), language=\"python\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16",
   "metadata": {},
   "source": [
    "## Prompt Engineering as Software in Practice\n",
    "\n",
    "The prompt template for experiment review is a YAML file with explicit metadata, structured dimensions, and Jinja2 template variables. We read it directly from the package:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "reviewer = ExperimentReviewer()\n",
    "template_path = reviewer.prompt_template_dir() / \"experiment_review.yaml\"\n",
    "print(template_path.read_text())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18",
   "metadata": {},
   "source": [
    "Knowledge files ground the LLM's assessment in documented domain expertise. Each file encodes a specific aspect of experimental design:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": [
    "knowledge_dir = reviewer.knowledge_content_dir()\n",
    "for path in sorted(knowledge_dir.iterdir()):\n",
    "    if path.suffix in (\".md\", \".txt\"):\n",
    "        lines = path.read_text().splitlines()\n",
    "        print(f\"--- {path.name} ({len(lines)} lines) ---\")\n",
    "        print(\"\\n\".join(lines[:8]))\n",
    "        print(\"...\")\n",
    "        print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20",
   "metadata": {},
   "source": [
    "## Layered Specialization in Practice\n",
    "\n",
    "The `MethodReviewer` abstract base class defines the interface that all reviewers must satisfy. Concrete subclasses provide method-specific implementations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {},
   "outputs": [],
   "source": [
    "Code(inspect.getsource(MethodReviewer), language=\"python\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22",
   "metadata": {},
   "source": [
    "## Structured Output in Practice\n",
    "\n",
    "The `ReviewResult` and `ReviewDimension` dataclasses define the typed output structure that the parsing logic assembles from the LLM's free-form response:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23",
   "metadata": {},
   "outputs": [],
   "source": [
    "Code(inspect.getsource(ReviewResult), language=\"python\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24",
   "metadata": {},
   "outputs": [],
   "source": [
    "Code(inspect.getsource(ReviewDimension), language=\"python\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25",
   "metadata": {},
   "source": [
    "## Connecting the Patterns: Evaluate.execute\n",
    "\n",
    "The `Evaluate.execute` method shows how all four patterns compose into the full pipeline:\n",
    "\n",
    "1. **Registry dispatch**: `MethodReviewerRegistry.create(manifest.model_type)` selects the reviewer\n",
    "2. **Strategy dispatch**: branches on `\"deterministic\"` vs `\"agentic\"` — only the confidence source differs\n",
    "3. **Prompt templates + knowledge injection**: handled inside `review()`, using the reviewer's template and knowledge directories\n",
    "4. **Structured output**: `review()` returns a typed `ReviewResult`; the method returns `asdict(result)` — a typed dict with guaranteed keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26",
   "metadata": {},
   "outputs": [],
   "source": [
    "Code(inspect.getsource(Evaluate), language=\"python\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27",
   "metadata": {},
   "source": [
    "## Escalation and Assess vs. Improve in Practice\n",
    "\n",
    "The `Evaluate.execute` method currently implements the **Judge** pattern by default: a single LLM pass constrained to structured output. The `evaluate_strategy` field in the manifest selects between deterministic scoring (`\"deterministic\"`) and the full agentic reviewer (`\"agentic\"`), but both branches use a single-pass Judge.\n",
    "\n",
    "The escalation ladder and the Assess vs. Improve discipline are implemented *around* this code, not within it:\n",
    "\n",
    "- **Jury** would call `Evaluate.execute` with multiple different backend configurations and aggregate or compare their `ReviewResult` outputs.\n",
    "- **Reviewer** would call `Evaluate.execute` once, then pass the `ReviewResult` back to a critique pass that checks whether the justifications adequately address each diagnostic.\n",
    "- **Debate** would call `Evaluate.execute` twice with adversarial system prompts, then run a resolution step.\n",
    "\n",
    "The `confidence_map()` output seen above is relevant here: the confidence ranges reflect the *hierarchy of evidence* (a structural prior about methodology quality), while the escalation pattern choice reflects *how thoroughly to interrogate a specific estimate*. These are orthogonal decisions — you can run a Debate-level review on a quasi-experimental study, or stay at Judge for a well-powered RCT.\n",
    "\n",
    "The Assess vs. Improve discipline likewise operates around `Evaluate.execute`: Assess mode calls it repeatedly on synthetic artifacts with known properties (clean vs. flawed), records pass/fail per dimension, and produces a scorecard. Improve mode then modifies the prompt templates and rubrics based on patterns in that scorecard — and validates every change on held-out artifacts that were not part of the diagnosis. Lecture 3 demonstrates the Assess mode concretely: we run known-clean and known-flaw artifacts through the reviewer and verify that the system discriminates appropriately."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28",
   "metadata": {},
   "source": [
    "## Additional resources\n",
    "\n",
    "- [Anthropic: Building Effective Agents](https://www.anthropic.com/engineering/building-effective-agents) — Design patterns for LLM-powered systems\n",
    "- [Anthropic: Claude Agent SDK](https://docs.anthropic.com/en/docs/agents-and-tools/claude-agent-sdk/overview) — SDK for building agents with Claude"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
