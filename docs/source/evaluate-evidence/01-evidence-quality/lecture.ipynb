{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Evaluating Evidence Quality\n",
    "\n",
    "This lecture develops a framework for assessing whether causal estimates are trustworthy enough to act on. We then apply these concepts using the `impact-engine-evaluate` package, which automates evidence evaluation as the bridge between measuring impact and allocating resources."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part I: Theory\n",
    "\n",
    "Measuring causal effects is only the first step. Before using an estimate to guide a business decision, we need to ask: **How much should we trust this number?** This section develops the conceptual tools for answering that question."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {},
   "source": [
    "## 1. Evaluating Evidence in General\n",
    "\n",
    "### Internal vs. External Validity\n",
    "\n",
    "Every causal study faces two distinct questions:\n",
    "\n",
    "| Validity Type | Question | Threats |\n",
    "|---------------|----------|----------|\n",
    "| **Internal validity** | Is the causal estimate correct *for this study population*? | Selection bias, confounding, measurement error |\n",
    "| **External validity** | Does the estimate generalize *to other populations or settings*? | Sample selection, context dependence, time effects |\n",
    "\n",
    "Internal validity is a prerequisite — an estimate that is wrong for the study population tells us nothing about other settings. But a perfectly valid estimate from a narrow population may not apply elsewhere.\n",
    "\n",
    "### Statistical vs. Practical Significance\n",
    "\n",
    "A result can be statistically significant without being practically meaningful, and vice versa:\n",
    "\n",
    "- **Statistical significance** asks: Could this result have arisen by chance? (Measured by p-values, confidence intervals)\n",
    "- **Practical significance** asks: Is the effect large enough to matter for the decision? (Measured by effect sizes, cost-benefit analysis)\n",
    "\n",
    "With large samples, even trivially small effects become statistically significant. Conversely, small samples may fail to detect genuinely important effects. Decision-makers need both: evidence that the effect is real *and* that it is large enough to justify action.\n",
    "\n",
    "### Hierarchy of Evidence\n",
    "\n",
    "Not all research designs provide equally credible evidence. The strength of a causal claim depends on how effectively the design rules out alternative explanations:\n",
    "\n",
    "| Tier | Design | Strength |\n",
    "|------|--------|----------|\n",
    "| 1 | Randomized experiments (RCTs) | Random assignment eliminates selection bias by construction |\n",
    "| 2 | Quasi-experimental methods (DiD, RDD, IV) | Exploit natural variation but rely on untestable assumptions |\n",
    "| 3 | Observational methods (matching, regression) | Require strong conditional independence assumptions |\n",
    "\n",
    "Higher tiers are not always feasible. The goal is to use the strongest design available and to be transparent about the assumptions required by weaker designs.\n",
    "\n",
    "### Replication and Robustness\n",
    "\n",
    "A single estimate, no matter how well-designed the study, provides limited evidence. Confidence increases when:\n",
    "\n",
    "- **Replication**: Different studies, using different data and methods, reach similar conclusions\n",
    "- **Robustness**: The estimate remains stable across reasonable changes in specification, sample, or assumptions\n",
    "\n",
    "Sensitivity to minor analytical choices — such as which covariates to include or how to define the outcome — signals fragility in the evidence."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3",
   "metadata": {},
   "source": [
    "## 2. Diagnostics Shared Across Causal Methods\n",
    "\n",
    "Regardless of the specific causal method, several diagnostic tools apply broadly. These checks assess whether the core assumptions of the design are plausible.\n",
    "\n",
    "### Covariate Balance\n",
    "\n",
    "Causal methods that condition on observables (matching, subclassification, regression) rely on treated and control groups being comparable. **Covariate balance** measures how similar the groups are on pre-treatment characteristics.\n",
    "\n",
    "The standard metric is the **standardized mean difference (SMD)**:\n",
    "\n",
    "$$\\text{SMD}_k = \\frac{\\bar{X}_{k,\\text{treated}} - \\bar{X}_{k,\\text{control}}}{\\sqrt{(s_{k,\\text{treated}}^2 + s_{k,\\text{control}}^2) / 2}}$$\n",
    "\n",
    "A common threshold is $|\\text{SMD}| < 0.1$, though this is a guideline, not a rule. **Love plots** display SMDs for all covariates before and after adjustment, making it easy to assess whether a method has improved balance.\n",
    "\n",
    "```{note}\n",
    "We used Love plots in the matching and subclassification lecture to assess balance improvement. The evaluate tool checks covariate balance as part of its automated review.\n",
    "```\n",
    "\n",
    "### Placebo Tests\n",
    "\n",
    "**Placebo tests** apply the causal method in settings where we know the true effect is zero. If the method detects a \"significant\" effect where none exists, something is wrong.\n",
    "\n",
    "Common placebo tests include:\n",
    "\n",
    "| Type | Description | Example |\n",
    "|------|-------------|----------|\n",
    "| **Placebo outcomes** | Apply the method to outcomes that should not be affected by treatment | Treatment is a marketing campaign; placebo outcome is warehouse temperature |\n",
    "| **Placebo treatments** | Assign treatment at a time or threshold where no real treatment occurred | In a DiD design, test for a \"treatment effect\" two periods before the actual intervention |\n",
    "| **Placebo units** | Apply the method to units that were not actually treated | In synthetic control, run the method on a control unit and check if a gap appears |\n",
    "\n",
    "### Sensitivity Analysis\n",
    "\n",
    "Every causal method relies on assumptions that cannot be fully tested with the data. **Sensitivity analysis** asks: How much would the assumptions need to be violated to overturn the conclusion?\n",
    "\n",
    "- **Rosenbaum bounds**: For matching estimators, quantify how large an unobserved confounder would need to be to explain away the effect\n",
    "- **Coefficient stability**: If adding additional covariates substantially changes the estimate, the conditional independence assumption may be fragile\n",
    "- **Omitted variable bias bounds**: Formal frameworks (e.g., Oster 2019) bound the bias from unobserved confounders based on the explanatory power of observed ones\n",
    "\n",
    "### Common Support / Overlap\n",
    "\n",
    "Methods that compare treated and control units require that both groups exist across the relevant range of covariates. **Common support** (or **overlap**) means that for any covariate value, there is a positive probability of being in either group:\n",
    "\n",
    "$$0 < P(D=1 \\mid X=x) < 1 \\quad \\text{for all } x$$\n",
    "\n",
    "Violations of common support mean the method is extrapolating rather than comparing like with like. Diagnostics include propensity score histograms and trimming rules.\n",
    "\n",
    "### Pre-treatment Outcome Trends\n",
    "\n",
    "For methods that use time-series variation (difference-in-differences, synthetic control), **parallel pre-treatment trends** between treated and control units support the identifying assumption. If outcomes diverge before treatment, the estimated effect may reflect pre-existing differences rather than the intervention."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4",
   "metadata": {},
   "source": [
    "## 3. Method-Specific Diagnostics\n",
    "\n",
    "Beyond shared diagnostics, each causal method has its own set of checks tailored to its specific assumptions.\n",
    "\n",
    "### Experiments (RCTs)\n",
    "\n",
    "| Diagnostic | What It Checks | Red Flag |\n",
    "|------------|----------------|----------|\n",
    "| **Randomization integrity** | Covariate balance between treatment and control | Systematic differences in baseline characteristics |\n",
    "| **Attrition** | Whether dropout rates differ by treatment status | Differential attrition compromises random assignment |\n",
    "| **Non-compliance** | Whether all units received their assigned treatment | High non-compliance dilutes the estimated effect |\n",
    "| **Spillover / SUTVA** | Whether treatment of one unit affects others | Interference between units biases the estimate |\n",
    "| **Effect plausibility** | Whether the magnitude of the effect is realistic | Implausibly large effects suggest measurement or specification errors |\n",
    "\n",
    "### Matching & Subclassification\n",
    "\n",
    "| Diagnostic | What It Checks | Red Flag |\n",
    "|------------|----------------|----------|\n",
    "| **Balance improvement** | Whether matching reduced covariate imbalance | SMDs remain large after matching |\n",
    "| **Common support** | Whether treated and control overlap in covariate space | Many treated units have no comparable controls |\n",
    "| **Hidden bias sensitivity** | How large an unobserved confounder would need to be | Effect disappears at low values of Rosenbaum's $\\Gamma$ |\n",
    "\n",
    "### Synthetic Control\n",
    "\n",
    "| Diagnostic | What It Checks | Red Flag |\n",
    "|------------|----------------|----------|\n",
    "| **Pre-treatment fit (RMSPE)** | How well the synthetic control tracks the treated unit before treatment | Large pre-treatment gaps undermine post-treatment comparisons |\n",
    "| **Placebo gaps** | Whether placebo units show similar post-treatment gaps | Many placebos have gaps as large as the treated unit |\n",
    "| **RMSPE ratios** | Post/pre RMSPE ratio relative to placebo distribution | Treated unit's ratio is not extreme relative to placebos |\n",
    "| **Donor composition** | Whether the synthetic control relies on sensible weights | Weights concentrated on dissimilar units |\n",
    "\n",
    "### Other Methods (Preview)\n",
    "\n",
    "Methods covered later in the course have their own diagnostics:\n",
    "\n",
    "| Method | Key Diagnostics |\n",
    "|--------|------------------|\n",
    "| **Difference-in-Differences** | Parallel pre-trends, event study plots, staggered treatment tests |\n",
    "| **Instrumental Variables** | First-stage F-statistic, exclusion restriction arguments, overidentification tests |\n",
    "| **Regression Discontinuity** | Continuity of baseline covariates at cutoff, McCrary density test, bandwidth sensitivity |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part II: Application\n",
    "\n",
    "In Part I we developed a framework for evaluating causal evidence — from general principles of validity and significance, through shared diagnostics like covariate balance and placebo tests, to method-specific checks for experiments, matching, and synthetic control.\n",
    "\n",
    "In this application we use the `impact-engine-evaluate` package, which automates this assessment as the bridge between the MEASURE and ALLOCATE stages of the decision pipeline. The tool implements two evaluation strategies: deterministic confidence scoring based on methodology, and agentic review where an LLM applies the diagnostic framework from Part I to actual measurement artifacts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import inspect\n",
    "\n",
    "from impact_engine_evaluate import Evaluate, score_confidence\n",
    "from impact_engine_evaluate.review.methods.base import MethodReviewerRegistry\n",
    "from impact_engine_evaluate.review.methods.experiment.reviewer import ExperimentReviewer\n",
    "from impact_engine_evaluate.review.models import ReviewDimension, ReviewResult\n",
    "from IPython.display import Code\n",
    "\n",
    "from support import (\n",
    "    create_mock_job_directory,\n",
    "    plot_confidence_ranges,\n",
    "    plot_review_dimensions,\n",
    "    print_evaluate_result,\n",
    "    print_review_result,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7",
   "metadata": {},
   "source": [
    "## 1. Pipeline Context\n",
    "\n",
    "The decision pipeline flows through three stages:\n",
    "\n",
    "$$\\text{MEASURE} \\;\\longrightarrow\\; \\text{EVALUATE} \\;\\longrightarrow\\; \\text{ALLOCATE}$$\n",
    "\n",
    "- **MEASURE** produces causal estimates (effect sizes, confidence intervals, diagnostics)\n",
    "- **EVALUATE** assesses how trustworthy those estimates are and assigns a confidence score\n",
    "- **ALLOCATE** uses the confidence-weighted estimates to decide where to invest resources\n",
    "\n",
    "The `Evaluate` adapter orchestrates the evaluation stage. It reads a job directory produced by MEASURE, dispatches on the configured strategy, and outputs a standardized 8-key dictionary that ALLOCATE consumes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "Code(inspect.getsource(Evaluate), language=\"python\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9",
   "metadata": {},
   "source": [
    "### Interface-to-Theory Mapping\n",
    "\n",
    "The `Evaluate` adapter connects Part I concepts to concrete implementation:\n",
    "\n",
    "| Adapter Field / Method | Part I Concept |\n",
    "|------------------------|----------------|\n",
    "| `evaluate_strategy` | Choice between methodology-based priors (Section 1: hierarchy of evidence) and detailed diagnostic review (Sections 2–3) |\n",
    "| `confidence` output | Overall trustworthiness — combines statistical and practical significance |\n",
    "| `MethodReviewerRegistry` | Method-specific diagnostics (Section 3) — each reviewer encodes the relevant checks for its methodology |\n",
    "| `confidence_range` per method | Hierarchy of evidence (Section 1) — experiments get higher baseline confidence than observational methods |\n",
    "| Review dimensions | Maps directly to diagnostic categories: randomization integrity, specification adequacy, statistical inference, threats to validity, effect plausibility |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10",
   "metadata": {},
   "source": [
    "## 2. Deterministic Scoring\n",
    "\n",
    "The simplest evaluation strategy assigns a confidence score based on the **methodology used**, without examining the specific results. This reflects the hierarchy of evidence from Part I: an experiment, by design, provides stronger evidence than an observational study.\n",
    "\n",
    "### Registered Methods and Confidence Ranges\n",
    "\n",
    "Each registered method reviewer defines a confidence range reflecting the methodology's inherent strength:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show registered methods and their confidence ranges\n",
    "confidence_map = MethodReviewerRegistry.confidence_map()\n",
    "\n",
    "for method, (lower, upper) in confidence_map.items():\n",
    "    print(f\"  {method}: [{lower:.2f}, {upper:.2f}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_confidence_ranges(confidence_map)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13",
   "metadata": {},
   "source": [
    "The confidence range for experiments (0.85–1.00) is higher than it would be for observational methods, reflecting the stronger identification strategy. Within each range, the exact score is drawn deterministically from the initiative ID, ensuring reproducibility.\n",
    "\n",
    "### Scoring an Initiative\n",
    "\n",
    "To demonstrate the scoring strategy, we create a mock job directory that mimics the output of the MEASURE stage:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "Code(inspect.getsource(create_mock_job_directory), language=\"python\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create mock MEASURE output\n",
    "job_dir = create_mock_job_directory()\n",
    "\n",
    "# Show the manifest\n",
    "import json\n",
    "\n",
    "manifest = json.loads((job_dir / \"manifest.json\").read_text())\n",
    "print(json.dumps(manifest, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16",
   "metadata": {},
   "source": [
    "Now we run the EVALUATE stage. The `Evaluate` adapter reads the manifest, identifies the method reviewer, and dispatches to the score strategy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the full EVALUATE pipeline\n",
    "evaluator = Evaluate()\n",
    "result = evaluator.execute({\"job_dir\": str(job_dir)})\n",
    "\n",
    "print_evaluate_result(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18",
   "metadata": {},
   "source": [
    "The `score_confidence` function can also be called directly with an initiative ID and confidence range:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Direct scoring — same deterministic confidence for the same initiative ID\n",
    "score_result = score_confidence(\"initiative_product_content_experiment\", (0.85, 1.0))\n",
    "print(f\"Confidence: {score_result.confidence:.3f}\")\n",
    "print(f\"Range:      {score_result.confidence_range}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20",
   "metadata": {},
   "source": [
    "## 3. Agentic Review\n",
    "\n",
    "The deterministic strategy assigns confidence based on methodology alone. The **agentic review** strategy goes further: it sends the actual measurement artifacts to an LLM, which evaluates them against the diagnostic framework from Part I.\n",
    "\n",
    "### Review Dimensions\n",
    "\n",
    "The `ExperimentReviewer` defines five review dimensions, each corresponding to a diagnostic category from Part I Section 3:\n",
    "\n",
    "| Review Dimension | Part I Diagnostic | What the LLM Assesses |\n",
    "|------------------|-------------------|-----------------------|\n",
    "| Randomization integrity | Randomization integrity | Covariate balance, randomization procedure, baseline equivalence |\n",
    "| Specification adequacy | (Statistical inference) | OLS formula, covariate selection, functional form |\n",
    "| Statistical inference | Statistical significance | Confidence intervals, p-values, standard errors |\n",
    "| Threats to validity | Attrition, non-compliance, spillover, SUTVA | Whether common threats are present or addressed |\n",
    "| Effect size plausibility | Effect plausibility | Whether the magnitude is realistic for the intervention |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {},
   "outputs": [],
   "source": [
    "Code(inspect.getsource(ExperimentReviewer), language=\"python\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22",
   "metadata": {},
   "source": [
    "### Knowledge Base\n",
    "\n",
    "Each method reviewer maintains a knowledge directory with domain expertise files that ground the LLM's assessment. For experiments, this includes design principles, common pitfalls, and diagnostic standards:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show the knowledge files available to the experiment reviewer\n",
    "reviewer = ExperimentReviewer()\n",
    "knowledge_dir = reviewer.knowledge_content_dir()\n",
    "\n",
    "for path in sorted(knowledge_dir.iterdir()):\n",
    "    if path.suffix in (\".md\", \".txt\"):\n",
    "        line_count = len(path.read_text().splitlines())\n",
    "        print(f\"  {path.name} ({line_count} lines)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24",
   "metadata": {},
   "source": [
    "### Pre-computed Review Output\n",
    "\n",
    "```{note}\n",
    "The agentic review calls an LLM API, which requires an API key and incurs cost. Since this notebook runs during the documentation build (where no API key is available), we construct a representative `ReviewResult` from pre-computed values. In practice, calling `Evaluate.execute()` with `evaluate_strategy: \"review\"` in the manifest produces this output automatically.\n",
    "```\n",
    "\n",
    "The following shows what a typical agentic review produces for a well-designed experiment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pre-computed representative review output\n",
    "review_result = ReviewResult(\n",
    "    initiative_id=\"initiative_product_content_experiment\",\n",
    "    prompt_name=\"experiment_review\",\n",
    "    prompt_version=\"1.0\",\n",
    "    backend_name=\"anthropic\",\n",
    "    model=\"claude-sonnet-4-5-20250929\",\n",
    "    dimensions=[\n",
    "        ReviewDimension(\n",
    "            name=\"randomization_integrity\",\n",
    "            score=0.92,\n",
    "            justification=(\n",
    "                \"Covariate balance is strong (max SMD = 0.04). Random assignment \"\n",
    "                \"appears properly implemented with no systematic baseline differences.\"\n",
    "            ),\n",
    "        ),\n",
    "        ReviewDimension(\n",
    "            name=\"specification_adequacy\",\n",
    "            score=0.85,\n",
    "            justification=(\n",
    "                \"Standard OLS specification with treatment indicator. Could benefit \"\n",
    "                \"from covariate adjustment to improve precision, but the core specification is sound.\"\n",
    "            ),\n",
    "        ),\n",
    "        ReviewDimension(\n",
    "            name=\"statistical_inference\",\n",
    "            score=0.88,\n",
    "            justification=(\n",
    "                \"Confidence interval [80, 220] excludes zero. p-value of 0.003 indicates \"\n",
    "                \"statistical significance. Sample size of 500 provides adequate power.\"\n",
    "            ),\n",
    "        ),\n",
    "        ReviewDimension(\n",
    "            name=\"threats_to_validity\",\n",
    "            score=0.80,\n",
    "            justification=(\n",
    "                \"Attrition rate of 5% is acceptable. Compliance rate of 92% is high. \"\n",
    "                \"No evidence of spillover, though SUTVA cannot be fully verified from artifacts alone.\"\n",
    "            ),\n",
    "        ),\n",
    "        ReviewDimension(\n",
    "            name=\"effect_size_plausibility\",\n",
    "            score=0.83,\n",
    "            justification=(\n",
    "                \"Effect estimate of $150 (roughly 30% of baseline) is plausible for a \"\n",
    "                \"content optimization intervention, though on the higher end of typical effects.\"\n",
    "            ),\n",
    "        ),\n",
    "    ],\n",
    "    overall_score=0.856,\n",
    "    raw_response=\"(pre-computed for documentation build)\",\n",
    "    timestamp=\"2026-01-15T10:30:00Z\",\n",
    ")\n",
    "\n",
    "print_review_result(review_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_review_dimensions(review_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27",
   "metadata": {},
   "source": [
    "## 4. From Confidence to Allocation\n",
    "\n",
    "Both evaluation strategies — deterministic scoring and agentic review — produce the same 8-key output dictionary. This standardized interface is what the downstream ALLOCATE stage consumes:\n",
    "\n",
    "| Key | Type | Description |\n",
    "|-----|------|-------------|\n",
    "| `initiative_id` | str | Unique identifier for the initiative |\n",
    "| `confidence` | float | Trustworthiness score (0–1) |\n",
    "| `cost` | float | Cost to scale the initiative |\n",
    "| `return_best` | float | Upper bound of expected return (CI upper) |\n",
    "| `return_median` | float | Point estimate of return |\n",
    "| `return_worst` | float | Lower bound of expected return (CI lower) |\n",
    "| `model_type` | str | Causal method used |\n",
    "| `sample_size` | int | Number of observations |\n",
    "\n",
    "### How Confidence Discounts Returns\n",
    "\n",
    "The ALLOCATE stage uses the confidence score to discount expected returns. An initiative with high measured impact but low confidence receives a smaller allocation than one with moderate impact and high confidence:\n",
    "\n",
    "$$\\text{Adjusted Return} = \\text{confidence} \\times \\text{return\\_median}$$\n",
    "\n",
    "This captures the key insight from Part I: the *quality* of evidence matters as much as the *magnitude* of the estimate. A large but unreliable effect is worth less than a moderate but well-established one.\n",
    "\n",
    "For example, with our experiment's output:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate confidence discounting\n",
    "confidence = result[\"confidence\"]\n",
    "return_median = result[\"return_median\"]\n",
    "adjusted_return = confidence * return_median\n",
    "\n",
    "print(f\"Raw return estimate:    ${return_median:,.0f}\")\n",
    "print(f\"Confidence score:       {confidence:.3f}\")\n",
    "print(f\"Adjusted return:        ${adjusted_return:,.0f}\")\n",
    "print()\n",
    "print(\"Compare with a hypothetical observational study:\")\n",
    "obs_confidence = 0.55\n",
    "obs_return = 200.0\n",
    "print(f\"  Raw return estimate:  ${obs_return:,.0f}\")\n",
    "print(f\"  Confidence score:     {obs_confidence:.3f}\")\n",
    "print(f\"  Adjusted return:      ${obs_confidence * obs_return:,.0f}\")\n",
    "print()\n",
    "print(\"The experiment's adjusted return is higher despite a lower raw estimate,\")\n",
    "print(\"because the stronger methodology commands greater confidence.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29",
   "metadata": {},
   "source": [
    "## Additional resources\n",
    "\n",
    "- **Angrist, J. D. & Pischke, J.‑S. (2010)**. The credibility revolution in empirical economics: How better research design is taking the con out of econometrics. *Journal of Economic Perspectives*, 24(2), 3–30.\n",
    "\n",
    "- **Athey, S. & Imbens, G. W. (2017)**. [The econometrics of randomized experiments](https://doi.org/10.1016/bs.hefe.2016.10.003). In *Handbook of Economic Field Experiments* (Vol. 1, pp. 73–140). Elsevier.\n",
    "\n",
    "- **Imbens, G. W. (2020)**. [Potential outcome and directed acyclic graph approaches to causality: Relevance for empirical practice in economics](https://www.aeaweb.org/articles?id=10.1257/jel.20191597). *Journal of Economic Literature*, 58(4), 1129–1179.\n",
    "\n",
    "- **Oster, E. (2019)**. Unobservable selection and coefficient stability: Theory and evidence. *Journal of Business & Economic Statistics*, 37(2), 187–204.\n",
    "\n",
    "- **Rosenbaum, P. R. (2002)**. *Observational Studies* (2nd ed.). Springer.\n",
    "\n",
    "- **Young, A. (2022)**. Consistency without inference: Instrumental variables in practical application. *European Economic Review*, 147, 104112."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
