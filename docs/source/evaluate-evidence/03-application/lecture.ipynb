{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Automated Evidence Review\n",
    "\n",
    "In Lecture 1 we developed the diagnostic framework for evaluating causal evidence; in Lecture 2 we examined the design patterns that power the evaluation tool. This lecture puts both together: we use the `impact-engine-evaluate` package end-to-end, running the full MEASURE → EVALUATE → ALLOCATE pipeline to demonstrate how evidence quality translates into investment decisions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part I: Theory\n",
    "\n",
    "The decision pipeline flows through three stages:\n",
    "\n",
    "$$\\text{MEASURE} \\;\\longrightarrow\\; \\text{EVALUATE} \\;\\longrightarrow\\; \\text{ALLOCATE}$$\n",
    "\n",
    "- **MEASURE** produces causal estimates: effect sizes, confidence intervals, and diagnostic statistics\n",
    "- **EVALUATE** assesses how trustworthy those estimates are and assigns a confidence score\n",
    "- **ALLOCATE** uses confidence-weighted estimates to decide where to invest resources\n",
    "\n",
    "The EVALUATE stage implements two strategies that correspond to different levels of evidence scrutiny:\n",
    "\n",
    "| Strategy | Basis | When to Use |\n",
    "|----------|-------|-------------|\n",
    "| `deterministic` | Methodology-based prior (hierarchy of evidence from Lecture 1) | Early screening, large portfolios, time-constrained decisions |\n",
    "| `agentic` | LLM diagnostic review (applying the framework from Lecture 1 to actual artifacts) | High-stakes decisions, detailed audit trail, before major resource commitments |\n",
    "\n",
    "Both strategies return the same 8-key output, making them interchangeable from the perspective of the downstream ALLOCATE stage."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part II: Application"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard Library\n",
    "import inspect\n",
    "import json\n",
    "\n",
    "# Third-party\n",
    "from impact_engine_evaluate import Evaluate, score_initiative\n",
    "from impact_engine_evaluate.review.methods.base import MethodReviewerRegistry\n",
    "from impact_engine_evaluate.review.models import ReviewDimension, ReviewResult\n",
    "from IPython.display import Code\n",
    "\n",
    "# Local\n",
    "from support import (\n",
    "    create_mock_job_directory,\n",
    "    plot_confidence_ranges,\n",
    "    plot_review_dimensions,\n",
    "    plot_score_comparison,\n",
    "    print_evaluate_result,\n",
    "    print_review_result,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4",
   "metadata": {},
   "source": [
    "## 1. Measurement Artifacts\n",
    "\n",
    "The EVALUATE stage reads a **job directory** produced by MEASURE. The directory contains two files:\n",
    "\n",
    "- `manifest.json`: describes the initiative, causal method, and evaluation strategy\n",
    "- `impact_results.json`: the measurement output — effect estimate, confidence interval, sample size, cost\n",
    "\n",
    "We use a helper function to create a mock job directory that mimics MEASURE output, so we can demonstrate EVALUATE without running the full pipeline:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "Code(inspect.getsource(create_mock_job_directory), language=\"python\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create mock MEASURE output\n",
    "job_dir = create_mock_job_directory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect the manifest\n",
    "manifest = json.loads((job_dir / \"manifest.json\").read_text())\n",
    "print(\"manifest.json:\")\n",
    "print(json.dumps(manifest, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect the impact results\n",
    "impact_results = json.loads((job_dir / \"impact_results.json\").read_text())\n",
    "print(\"impact_results.json:\")\n",
    "print(json.dumps(impact_results, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9",
   "metadata": {},
   "source": [
    "## 2. Deterministic Scoring\n",
    "\n",
    "The simplest evaluation strategy assigns a confidence score based on the **methodology used**, without examining the specific results. This reflects the hierarchy of evidence from Lecture 1: an experiment, by design, provides stronger evidence than an observational study.\n",
    "\n",
    "### Registered Methods and Confidence Ranges\n",
    "\n",
    "Each registered method reviewer defines a confidence range reflecting the methodology's inherent strength:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "confidence_map = MethodReviewerRegistry.confidence_map()\n",
    "\n",
    "for method, (lo, hi) in confidence_map.items():\n",
    "    print(f\"  {method}: [{lo:.2f}, {hi:.2f}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "Code(inspect.getsource(plot_confidence_ranges), language=\"python\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_confidence_ranges(confidence_map)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13",
   "metadata": {},
   "source": [
    "The confidence range for experiments (0.85–1.00) is higher than it would be for observational methods, reflecting the stronger identification strategy. Within each range, the exact score is drawn deterministically from the initiative ID, ensuring reproducibility across runs.\n",
    "\n",
    "### Running the EVALUATE Stage\n",
    "\n",
    "We run the full EVALUATE pipeline by passing the job directory to the `Evaluate` adapter. It reads the manifest, dispatches to the appropriate reviewer, and returns the standardized 8-key output:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "Code(inspect.getsource(print_evaluate_result), language=\"python\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator = Evaluate()\n",
    "result = evaluator.execute({\"job_dir\": str(job_dir)})\n",
    "\n",
    "print_evaluate_result(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16",
   "metadata": {},
   "source": [
    "`score_initiative` can also be called directly with an event dict and a confidence range — useful when you want to score a single initiative without reading a full job directory:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "direct_result = score_initiative(\n",
    "    {\n",
    "        \"initiative_id\": \"initiative_product_content_experiment\",\n",
    "        \"model_type\": \"experiment\",\n",
    "        \"effect_estimate\": 150.0,\n",
    "        \"ci_upper\": 220.0,\n",
    "        \"ci_lower\": 80.0,\n",
    "        \"cost_to_scale\": 50000.0,\n",
    "        \"sample_size\": 500,\n",
    "    },\n",
    "    confidence_range=(0.85, 1.0),\n",
    ")\n",
    "print(f\"Confidence: {direct_result['confidence']:.3f}\")\n",
    "print(f\"Range:      (0.85, 1.00)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18",
   "metadata": {},
   "source": [
    "## 3. Agentic Review\n",
    "\n",
    "The deterministic strategy assigns confidence based on methodology alone. The **agentic review** strategy goes further: it sends the actual measurement artifacts to an LLM, which evaluates them against the diagnostic framework from Lecture 1.\n",
    "\n",
    "### Review Dimensions\n",
    "\n",
    "The `ExperimentReviewer` defines five review dimensions, each corresponding to a diagnostic category from Lecture 1:\n",
    "\n",
    "| Review Dimension | Lecture 1 Diagnostic | What the LLM Assesses |\n",
    "|------------------|----------------------|-----------------------|\n",
    "| Randomization integrity | RCT diagnostics — randomization integrity | Covariate balance, randomization procedure, baseline equivalence |\n",
    "| Specification adequacy | RCT diagnostics — specification | OLS formula, covariate selection, functional form |\n",
    "| Statistical inference | Shared diagnostics — statistical significance | Confidence intervals, p-values, standard errors |\n",
    "| Threats to validity | RCT diagnostics — attrition, non-compliance, spillover | Whether common threats are present or addressed |\n",
    "| Effect size plausibility | RCT diagnostics — effect plausibility | Whether the magnitude is realistic for the intervention |\n",
    "\n",
    "```{note}\n",
    "The agentic review calls an LLM API, which requires an API key and incurs cost. Since this notebook runs during the documentation build (where no API key is available), we construct a representative `ReviewResult` from pre-computed values. In practice, set `evaluate_strategy: agentic` in the manifest and call `Evaluate.execute()` to produce this output automatically.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19",
   "metadata": {},
   "source": [
    "### Reading the Review Dimension by Dimension\n",
    "\n",
    "The review result for our product content experiment can be read as a structured narrative, where each dimension score traces directly to a specific artifact value in the job directory. This is the traceability pillar from Lecture 1 in practice — every number is walkable.\n",
    "\n",
    "**Randomization integrity — 0.92.** The covariate balance diagnostic shows a maximum SMD of 0.04 across all baseline covariates, well within the 0.1 threshold. Random assignment appears properly implemented with no systematic baseline differences. The score is high but not perfect: with 500 observations, balance was verified but the randomization procedure itself was not independently audited.\n",
    "\n",
    "**Specification adequacy — 0.85.** The OLS specification uses a treatment indicator without covariate adjustment. Given the strong balance, this is sound — but including pre-treatment covariates as controls would tighten the standard error without biasing the estimate. The reviewer flags this as a missed precision opportunity rather than a validity concern.\n",
    "\n",
    "**Statistical inference — 0.88.** The confidence interval [80, 220] excludes zero, the p-value of 0.003 indicates robust statistical significance, and the sample size of 500 is adequate for detection. The interval is moderately wide (a factor of 2.75 from lower to upper bound), reflecting real uncertainty in the effect magnitude.\n",
    "\n",
    "**Threats to validity — 0.80.** A 5% attrition rate is acceptable; differential attrition would require separate investigation. The 92% compliance rate is high, meaning the intent-to-treat estimate closely approximates the treatment effect on the treated. SUTVA cannot be fully verified from the artifact alone — the reviewer notes this as an unresolved uncertainty rather than a confirmed violation.\n",
    "\n",
    "**Effect size plausibility — 0.83.** The $150 effect represents roughly 30% of baseline revenue. This is plausible for a content optimization intervention but sits on the higher end of typical effects. The reviewer flags it as a yellow rather than red — not implausible, but worth triangulating with prior evidence before treating as established.\n",
    "\n",
    "**Overall: 0.856.** This score reflects genuinely strong evidence — a properly randomized experiment with good balance, significant results, and manageable threats. The deductions are real: the wide CI, the unmeasured SUTVA risk, and the larger-than-typical effect size each reduce confidence below the top of the experiment range (1.00). Every deduction is traceable to a specific artifact value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {},
   "outputs": [],
   "source": [
    "Code(inspect.getsource(print_review_result), language=\"python\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pre-computed representative review output\n",
    "review_result = ReviewResult(\n",
    "    initiative_id=\"initiative_product_content_experiment\",\n",
    "    prompt_name=\"experiment_review\",\n",
    "    prompt_version=\"1.0\",\n",
    "    backend_name=\"anthropic\",\n",
    "    model=\"claude-sonnet-4-6\",\n",
    "    dimensions=[\n",
    "        ReviewDimension(\n",
    "            name=\"randomization_integrity\",\n",
    "            score=0.92,\n",
    "            justification=(\n",
    "                \"Covariate balance is strong (max SMD = 0.04). Random assignment \"\n",
    "                \"appears properly implemented with no systematic baseline differences.\"\n",
    "            ),\n",
    "        ),\n",
    "        ReviewDimension(\n",
    "            name=\"specification_adequacy\",\n",
    "            score=0.85,\n",
    "            justification=(\n",
    "                \"Standard OLS specification with treatment indicator. Could benefit \"\n",
    "                \"from covariate adjustment to improve precision, but the core specification is sound.\"\n",
    "            ),\n",
    "        ),\n",
    "        ReviewDimension(\n",
    "            name=\"statistical_inference\",\n",
    "            score=0.88,\n",
    "            justification=(\n",
    "                \"Confidence interval [80, 220] excludes zero. p-value of 0.003 indicates \"\n",
    "                \"statistical significance. Sample size of 500 provides adequate power.\"\n",
    "            ),\n",
    "        ),\n",
    "        ReviewDimension(\n",
    "            name=\"threats_to_validity\",\n",
    "            score=0.80,\n",
    "            justification=(\n",
    "                \"Attrition rate of 5% is acceptable. Compliance rate of 92% is high. \"\n",
    "                \"No evidence of spillover, though SUTVA cannot be fully verified from artifacts alone.\"\n",
    "            ),\n",
    "        ),\n",
    "        ReviewDimension(\n",
    "            name=\"effect_size_plausibility\",\n",
    "            score=0.83,\n",
    "            justification=(\n",
    "                \"Effect estimate of $150 (roughly 30% of baseline) is plausible for a \"\n",
    "                \"content optimization intervention, though on the higher end of typical effects.\"\n",
    "            ),\n",
    "        ),\n",
    "    ],\n",
    "    overall_score=0.856,\n",
    "    raw_response=\"(pre-computed for documentation build)\",\n",
    "    timestamp=\"2026-01-15T10:30:00Z\",\n",
    ")\n",
    "\n",
    "print_review_result(review_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22",
   "metadata": {},
   "outputs": [],
   "source": [
    "Code(inspect.getsource(plot_review_dimensions), language=\"python\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_review_dimensions(review_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24",
   "metadata": {},
   "source": [
    "## 4. From Confidence to Allocation\n",
    "\n",
    "Both evaluation strategies produce the same 8-key output dictionary. This standardized interface is what the downstream ALLOCATE stage consumes:\n",
    "\n",
    "| Key | Type | Description |\n",
    "|-----|------|-------------|\n",
    "| `initiative_id` | str | Unique identifier for the initiative |\n",
    "| `confidence` | float | Trustworthiness score (0–1) |\n",
    "| `cost` | float | Cost to scale the initiative |\n",
    "| `return_best` | float | Upper bound of expected return (CI upper) |\n",
    "| `return_median` | float | Point estimate of return |\n",
    "| `return_worst` | float | Lower bound of expected return (CI lower) |\n",
    "| `model_type` | str | Causal method used |\n",
    "| `sample_size` | int | Number of observations |\n",
    "\n",
    "### How Confidence Discounts Returns\n",
    "\n",
    "The ALLOCATE stage uses the confidence score to discount expected returns. An initiative with high measured impact but low confidence receives a smaller allocation than one with moderate impact and high confidence:\n",
    "\n",
    "$$\\text{Adjusted Return} = \\text{confidence} \\times \\text{return\\_median}$$\n",
    "\n",
    "This captures the key insight from Lecture 1: the *quality* of evidence matters as much as the *magnitude* of the estimate. A large but unreliable effect is worth less than a moderate but well-established one.\n",
    "\n",
    "### The Organizational Incentive\n",
    "\n",
    "Confidence-weighted allocation creates a direct incentive for teams to invest in better measurement designs. Teams that run proper experiments, collect adequate samples, and perform thorough robustness checks receive higher confidence scores — and higher scores amplify their return estimates in the allocator. Teams relying on weak quasi-experimental designs or sparse data see their estimates discounted, even when the raw point estimates are large.\n",
    "\n",
    "This makes evidence quality a competitive advantage within the portfolio, not merely a methodological nicety. The allocation mechanism rewards measurement rigor in the same currency as it rewards impact magnitude."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25",
   "metadata": {},
   "outputs": [],
   "source": [
    "confidence = result[\"confidence\"]\n",
    "return_median = result[\"return_median\"]\n",
    "adjusted_return = confidence * return_median\n",
    "\n",
    "print(f\"Raw return estimate:    ${return_median:,.0f}\")\n",
    "print(f\"Confidence score:       {confidence:.3f}\")\n",
    "print(f\"Adjusted return:        ${adjusted_return:,.0f}\")\n",
    "print()\n",
    "print(\"Compare with a hypothetical observational study:\")\n",
    "# Typical observational-study confidence and a slightly larger raw estimate\n",
    "# illustrate how methodology discounting reverses the ranking.\n",
    "hypothetical_obs_confidence = 0.55\n",
    "hypothetical_obs_return = 200.0\n",
    "print(f\"  Raw return estimate:  ${hypothetical_obs_return:,.0f}\")\n",
    "print(f\"  Confidence score:     {hypothetical_obs_confidence:.3f}\")\n",
    "print(f\"  Adjusted return:      ${hypothetical_obs_confidence * hypothetical_obs_return:,.0f}\")\n",
    "print()\n",
    "print(\"The experiment's adjusted return is higher despite a lower raw estimate,\")\n",
    "print(\"because the stronger methodology commands greater confidence.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26",
   "metadata": {},
   "source": [
    "## 5. Evaluating the Evaluator\n",
    "\n",
    "The four pillars establish what a trustworthy evaluation system should look like. But the Correctness pillar is different from the others: you cannot guarantee it by design. You must test for it.\n",
    "\n",
    "Groundedness is enforced architecturally — the LLM only sees artifacts the measurement engine produced. Traceability is enforced by the per-dimension output schema. Reproducibility is enforced by fixed prompts, zero temperature, and version-pinned backends. But Correctness — whether the LLM accurately reads the evidence it is given — is an empirical property that must be verified through evaluation.\n",
    "\n",
    "The Assess mode applies the internal/external validity distinction from Lecture 1 to the automated system itself:\n",
    "\n",
    "**Internal validity** tests coherence without requiring ground truth. A well-specified system should produce stable scores regardless of which backend processes the artifact or how the prompt is phrased. If scores shift substantially across semantically equivalent prompts, the rubric is under-specified.\n",
    "\n",
    "**External validity** tests correctness against synthetic artifacts where the right answer is known by construction. A known-clean artifact — a well-powered RCT with excellent diagnostics — should score highly. A known-flaw artifact — the same method with deliberately poor diagnostics — should score lower and flag the specific problems. If the system cannot discriminate between them, the rubric fails its core purpose.\n",
    "\n",
    "We demonstrate both below using pre-computed `ReviewResult` objects representing the two extremes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Known-clean: well-powered RCT with excellent diagnostics — should score near the top of the range\n",
    "review_clean = ReviewResult(\n",
    "    initiative_id=\"eval_test_clean_rct\",\n",
    "    prompt_name=\"experiment_review\",\n",
    "    prompt_version=\"1.0\",\n",
    "    backend_name=\"anthropic\",\n",
    "    model=\"claude-sonnet-4-6\",\n",
    "    dimensions=[\n",
    "        ReviewDimension(\n",
    "            name=\"randomization_integrity\",\n",
    "            score=0.95,\n",
    "            justification=(\n",
    "                \"Perfect balance across all covariates (max SMD = 0.02). Randomization \"\n",
    "                \"procedure documented and independently verified.\"\n",
    "            ),\n",
    "        ),\n",
    "        ReviewDimension(\n",
    "            name=\"specification_adequacy\",\n",
    "            score=0.90,\n",
    "            justification=(\n",
    "                \"OLS with pre-specified covariates. No post-hoc specification changes. \"\n",
    "                \"Analysis plan registered before data collection.\"\n",
    "            ),\n",
    "        ),\n",
    "        ReviewDimension(\n",
    "            name=\"statistical_inference\",\n",
    "            score=0.93,\n",
    "            justification=(\n",
    "                \"CI [120, 180] tightly excludes zero. n = 10,000 provides high power. \"\n",
    "                \"p-value of 0.0001 indicates robust significance.\"\n",
    "            ),\n",
    "        ),\n",
    "        ReviewDimension(\n",
    "            name=\"threats_to_validity\",\n",
    "            score=0.92,\n",
    "            justification=(\n",
    "                \"Attrition 2% and symmetric across arms. Compliance 97%. \"\n",
    "                \"No evidence of spillover in available diagnostics.\"\n",
    "            ),\n",
    "        ),\n",
    "        ReviewDimension(\n",
    "            name=\"effect_size_plausibility\",\n",
    "            score=0.88,\n",
    "            justification=(\n",
    "                \"Effect of $50 (~10% of baseline) is consistent with prior literature \"\n",
    "                \"on content optimization interventions.\"\n",
    "            ),\n",
    "        ),\n",
    "    ],\n",
    "    overall_score=0.916,\n",
    "    raw_response=\"(pre-computed for documentation build)\",\n",
    "    timestamp=\"2026-01-15T11:00:00Z\",\n",
    ")\n",
    "\n",
    "print_review_result(review_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Known-flaw: same method, deliberately poor diagnostics — should score lower and flag specific issues\n",
    "review_flawed = ReviewResult(\n",
    "    initiative_id=\"eval_test_flawed_rct\",\n",
    "    prompt_name=\"experiment_review\",\n",
    "    prompt_version=\"1.0\",\n",
    "    backend_name=\"anthropic\",\n",
    "    model=\"claude-sonnet-4-6\",\n",
    "    dimensions=[\n",
    "        ReviewDimension(\n",
    "            name=\"randomization_integrity\",\n",
    "            score=0.42,\n",
    "            justification=(\n",
    "                \"Three covariates exceed SMD 0.1 (max SMD = 0.35). Baseline balance is \"\n",
    "                \"compromised — randomization may have failed or was not properly implemented.\"\n",
    "            ),\n",
    "        ),\n",
    "        ReviewDimension(\n",
    "            name=\"specification_adequacy\",\n",
    "            score=0.65,\n",
    "            justification=(\n",
    "                \"Imbalanced covariates not included as controls, leaving known confounders \"\n",
    "                \"unaddressed. Specification does not adequately account for baseline differences.\"\n",
    "            ),\n",
    "        ),\n",
    "        ReviewDimension(\n",
    "            name=\"statistical_inference\",\n",
    "            score=0.55,\n",
    "            justification=(\n",
    "                \"CI [-5, 305] barely excludes zero at conventional thresholds. \"\n",
    "                \"Small sample (n = 80) suggests underpowered design; result is fragile.\"\n",
    "            ),\n",
    "        ),\n",
    "        ReviewDimension(\n",
    "            name=\"threats_to_validity\",\n",
    "            score=0.38,\n",
    "            justification=(\n",
    "                \"Attrition 25% with differential rates across arms (treated: 18%, control: 32%). \"\n",
    "                \"Compliance 68%. Spillover mechanisms present but unaddressed.\"\n",
    "            ),\n",
    "        ),\n",
    "        ReviewDimension(\n",
    "            name=\"effect_size_plausibility\",\n",
    "            score=0.60,\n",
    "            justification=(\n",
    "                \"Effect of $300 (~60% of baseline) is implausibly large for a content \"\n",
    "                \"optimization intervention. Possible measurement error or specification problem.\"\n",
    "            ),\n",
    "        ),\n",
    "    ],\n",
    "    overall_score=0.52,\n",
    "    raw_response=\"(pre-computed for documentation build)\",\n",
    "    timestamp=\"2026-01-15T11:01:00Z\",\n",
    ")\n",
    "\n",
    "print_review_result(review_flawed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29",
   "metadata": {},
   "outputs": [],
   "source": [
    "Code(inspect.getsource(plot_score_comparison), language=\"python\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_score_comparison(review_clean, review_flawed, labels=[\"Known-clean RCT\", \"Known-flaw RCT\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31",
   "metadata": {},
   "source": [
    "### What This Tells Us\n",
    "\n",
    "The score gap (0.916 vs. 0.52) confirms **external validity**: the reviewer discriminates sharply between strong and weak evidence. It does not cluster scores in a narrow band — the full range is used, and the directional ordering is correct.\n",
    "\n",
    "The dimension-level breakdown matters as much as the overall score. Threats to validity (0.38) is the most penalized dimension in the flawed case, driven by the combination of high differential attrition and low compliance — exactly the diagnostics that should dominate a validity assessment when randomization is compromised. Specification adequacy (0.65) is penalized less severely because the corollary failure (not controlling for imbalanced covariates) is a consequence of the randomization problem, not an independent error.\n",
    "\n",
    "Every low score is linked to a specific artifact value: max SMD = 0.35, attrition 25%, compliance 68%. This is the traceability pillar holding in the flawed case — not just in the clean one.\n",
    "\n",
    "### What This Implies for Improve Mode\n",
    "\n",
    "If the system had *failed* to penalize the known-flaw case — say, the threats to validity score came back as 0.75 instead of 0.38 — that would be a clear **external validity failure**. The Improve mode response would be to inspect the threats-to-validity prompt template and add explicit thresholds: \"An attrition rate above 15%, or differential attrition across arms, should substantially reduce this score.\" The fix would then be validated on held-out flawed artifacts — not the one that revealed the problem — before being promoted to production."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32",
   "metadata": {},
   "source": [
    "## Additional resources\n",
    "\n",
    "- **Young, A. (2022)**. Consistency without inference: Instrumental variables in practical application. *European Economic Review*, 147, 104112.\n",
    "\n",
    "- **Angrist, J. D. & Pischke, J.‑S. (2010)**. The credibility revolution in empirical economics: How better research design is taking the con out of econometrics. *Journal of Economic Perspectives*, 24(2), 3–30.\n",
    "\n",
    "- [impact-engine-evaluate documentation](https://eisenhauerio.github.io/tools-impact-engine-evaluate/) — Usage, configuration, and system design"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
