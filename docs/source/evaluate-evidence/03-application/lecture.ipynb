{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Automated Evidence Review\n",
    "\n",
    "In the previous lectures we developed the diagnostic framework for evaluating causal evidence and examined the design patterns that power the evaluation tool. This lecture puts both together: we use the `impact-engine-evaluate` package end-to-end, running the full MEASURE → EVALUATE → ALLOCATE pipeline to demonstrate how evidence quality translates into investment decisions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part I: Theory\n",
    "\n",
    "The decision pipeline flows through three stages:\n",
    "\n",
    "$$\\text{MEASURE} \\;\\longrightarrow\\; \\text{EVALUATE} \\;\\longrightarrow\\; \\text{ALLOCATE}$$\n",
    "\n",
    "- **MEASURE** produces causal estimates: effect sizes, confidence intervals, and diagnostic statistics\n",
    "- **EVALUATE** assesses how trustworthy those estimates are and assigns a confidence score\n",
    "- **ALLOCATE** uses confidence-weighted estimates to decide where to invest resources\n",
    "\n",
    "The EVALUATE stage implements two strategies that correspond to different levels of evidence scrutiny:\n",
    "\n",
    "| Strategy | Basis | When to Use |\n",
    "|----------|-------|-------------|\n",
    "| `score` | Methodology-based prior (hierarchy of evidence from Lecture 1) | Early screening, large portfolios, time-constrained decisions |\n",
    "| `review` | LLM diagnostic review (applying the framework from Lecture 1 to actual artifacts) | High-stakes decisions, detailed audit trail, before major resource commitments |\n",
    "\n",
    "Both strategies return the same 8-key output, making them interchangeable from the perspective of the downstream ALLOCATE stage."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part II: Application"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import inspect\n",
    "import json\n",
    "\n",
    "from impact_engine_evaluate import Evaluate, score_confidence\n",
    "from impact_engine_evaluate.review.methods.base import MethodReviewerRegistry\n",
    "from impact_engine_evaluate.review.models import ReviewDimension, ReviewResult\n",
    "from IPython.display import Code\n",
    "\n",
    "from support import (\n",
    "    create_mock_job_directory,\n",
    "    plot_confidence_ranges,\n",
    "    plot_review_dimensions,\n",
    "    print_evaluate_result,\n",
    "    print_review_result,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4",
   "metadata": {},
   "source": [
    "## 1. Measurement Artifacts\n",
    "\n",
    "The EVALUATE stage reads a **job directory** produced by MEASURE. The directory contains two files:\n",
    "\n",
    "- `manifest.json`: describes the initiative, causal method, and evaluation strategy\n",
    "- `impact_results.json`: the measurement output — effect estimate, confidence interval, sample size, cost\n",
    "\n",
    "We use a helper function to create a mock job directory that mimics MEASURE output, so we can demonstrate EVALUATE without running the full pipeline:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "Code(inspect.getsource(create_mock_job_directory), language=\"python\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create mock MEASURE output\n",
    "job_dir = create_mock_job_directory()\n",
    "\n",
    "# Inspect the manifest\n",
    "manifest = json.loads((job_dir / \"manifest.json\").read_text())\n",
    "print(\"manifest.json:\")\n",
    "print(json.dumps(manifest, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect the impact results\n",
    "impact_results = json.loads((job_dir / \"impact_results.json\").read_text())\n",
    "print(\"impact_results.json:\")\n",
    "print(json.dumps(impact_results, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8",
   "metadata": {},
   "source": [
    "## 2. Deterministic Scoring\n",
    "\n",
    "The simplest evaluation strategy assigns a confidence score based on the **methodology used**, without examining the specific results. This reflects the hierarchy of evidence from Lecture 1: an experiment, by design, provides stronger evidence than an observational study.\n",
    "\n",
    "### Registered Methods and Confidence Ranges\n",
    "\n",
    "Each registered method reviewer defines a confidence range reflecting the methodology's inherent strength:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "confidence_map = MethodReviewerRegistry.confidence_map()\n",
    "\n",
    "for method, (lo, hi) in confidence_map.items():\n",
    "    print(f\"  {method}: [{lo:.2f}, {hi:.2f}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_confidence_ranges(confidence_map)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11",
   "metadata": {},
   "source": [
    "The confidence range for experiments (0.85–1.00) is higher than it would be for observational methods, reflecting the stronger identification strategy. Within each range, the exact score is drawn deterministically from the initiative ID, ensuring reproducibility across runs.\n",
    "\n",
    "### Running the EVALUATE Stage\n",
    "\n",
    "We run the full EVALUATE pipeline by passing the job directory to the `Evaluate` adapter. It reads the manifest, dispatches to the appropriate reviewer, and returns the standardized 8-key output:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator = Evaluate()\n",
    "result = evaluator.execute({\"job_dir\": str(job_dir)})\n",
    "\n",
    "print_evaluate_result(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13",
   "metadata": {},
   "source": [
    "The `score_confidence` function can also be called directly with an initiative ID and confidence range — useful when you want to score without reading a job directory:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "score_result = score_confidence(\"initiative_product_content_experiment\", (0.85, 1.0))\n",
    "print(f\"Confidence: {score_result.confidence:.3f}\")\n",
    "print(f\"Range:      {score_result.confidence_range}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15",
   "metadata": {},
   "source": [
    "## 3. Agentic Review\n",
    "\n",
    "The deterministic strategy assigns confidence based on methodology alone. The **agentic review** strategy goes further: it sends the actual measurement artifacts to an LLM, which evaluates them against the diagnostic framework from Lecture 1.\n",
    "\n",
    "### Review Dimensions\n",
    "\n",
    "The `ExperimentReviewer` defines five review dimensions, each corresponding to a diagnostic category from Lecture 1:\n",
    "\n",
    "| Review Dimension | Lecture 1 Diagnostic | What the LLM Assesses |\n",
    "|------------------|----------------------|-----------------------|\n",
    "| Randomization integrity | RCT diagnostics — randomization integrity | Covariate balance, randomization procedure, baseline equivalence |\n",
    "| Specification adequacy | RCT diagnostics — specification | OLS formula, covariate selection, functional form |\n",
    "| Statistical inference | Shared diagnostics — statistical significance | Confidence intervals, p-values, standard errors |\n",
    "| Threats to validity | RCT diagnostics — attrition, non-compliance, spillover | Whether common threats are present or addressed |\n",
    "| Effect size plausibility | RCT diagnostics — effect plausibility | Whether the magnitude is realistic for the intervention |\n",
    "\n",
    "```{note}\n",
    "The agentic review calls an LLM API, which requires an API key and incurs cost. Since this notebook runs during the documentation build (where no API key is available), we construct a representative `ReviewResult` from pre-computed values. In practice, set `evaluate_strategy: review` in the manifest and call `Evaluate.execute()` to produce this output automatically.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pre-computed representative review output\n",
    "review_result = ReviewResult(\n",
    "    initiative_id=\"initiative_product_content_experiment\",\n",
    "    prompt_name=\"experiment_review\",\n",
    "    prompt_version=\"1.0\",\n",
    "    backend_name=\"anthropic\",\n",
    "    model=\"claude-sonnet-4-5-20250929\",\n",
    "    dimensions=[\n",
    "        ReviewDimension(\n",
    "            name=\"randomization_integrity\",\n",
    "            score=0.92,\n",
    "            justification=(\n",
    "                \"Covariate balance is strong (max SMD = 0.04). Random assignment \"\n",
    "                \"appears properly implemented with no systematic baseline differences.\"\n",
    "            ),\n",
    "        ),\n",
    "        ReviewDimension(\n",
    "            name=\"specification_adequacy\",\n",
    "            score=0.85,\n",
    "            justification=(\n",
    "                \"Standard OLS specification with treatment indicator. Could benefit \"\n",
    "                \"from covariate adjustment to improve precision, but the core specification is sound.\"\n",
    "            ),\n",
    "        ),\n",
    "        ReviewDimension(\n",
    "            name=\"statistical_inference\",\n",
    "            score=0.88,\n",
    "            justification=(\n",
    "                \"Confidence interval [80, 220] excludes zero. p-value of 0.003 indicates \"\n",
    "                \"statistical significance. Sample size of 500 provides adequate power.\"\n",
    "            ),\n",
    "        ),\n",
    "        ReviewDimension(\n",
    "            name=\"threats_to_validity\",\n",
    "            score=0.80,\n",
    "            justification=(\n",
    "                \"Attrition rate of 5% is acceptable. Compliance rate of 92% is high. \"\n",
    "                \"No evidence of spillover, though SUTVA cannot be fully verified from artifacts alone.\"\n",
    "            ),\n",
    "        ),\n",
    "        ReviewDimension(\n",
    "            name=\"effect_size_plausibility\",\n",
    "            score=0.83,\n",
    "            justification=(\n",
    "                \"Effect estimate of $150 (roughly 30% of baseline) is plausible for a \"\n",
    "                \"content optimization intervention, though on the higher end of typical effects.\"\n",
    "            ),\n",
    "        ),\n",
    "    ],\n",
    "    overall_score=0.856,\n",
    "    raw_response=\"(pre-computed for documentation build)\",\n",
    "    timestamp=\"2026-01-15T10:30:00Z\",\n",
    ")\n",
    "\n",
    "print_review_result(review_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_review_dimensions(review_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18",
   "metadata": {},
   "source": [
    "## 4. From Confidence to Allocation\n",
    "\n",
    "Both evaluation strategies produce the same 8-key output dictionary. This standardized interface is what the downstream ALLOCATE stage consumes:\n",
    "\n",
    "| Key | Type | Description |\n",
    "|-----|------|-------------|\n",
    "| `initiative_id` | str | Unique identifier for the initiative |\n",
    "| `confidence` | float | Trustworthiness score (0–1) |\n",
    "| `cost` | float | Cost to scale the initiative |\n",
    "| `return_best` | float | Upper bound of expected return (CI upper) |\n",
    "| `return_median` | float | Point estimate of return |\n",
    "| `return_worst` | float | Lower bound of expected return (CI lower) |\n",
    "| `model_type` | str | Causal method used |\n",
    "| `sample_size` | int | Number of observations |\n",
    "\n",
    "### How Confidence Discounts Returns\n",
    "\n",
    "The ALLOCATE stage uses the confidence score to discount expected returns. An initiative with high measured impact but low confidence receives a smaller allocation than one with moderate impact and high confidence:\n",
    "\n",
    "$$\\text{Adjusted Return} = \\text{confidence} \\times \\text{return\\_median}$$\n",
    "\n",
    "This captures the key insight from Lecture 1: the *quality* of evidence matters as much as the *magnitude* of the estimate. A large but unreliable effect is worth less than a moderate but well-established one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": [
    "confidence = result[\"confidence\"]\n",
    "return_median = result[\"return_median\"]\n",
    "adjusted_return = confidence * return_median\n",
    "\n",
    "print(f\"Raw return estimate:    ${return_median:,.0f}\")\n",
    "print(f\"Confidence score:       {confidence:.3f}\")\n",
    "print(f\"Adjusted return:        ${adjusted_return:,.0f}\")\n",
    "print()\n",
    "print(\"Compare with a hypothetical observational study:\")\n",
    "obs_confidence = 0.55\n",
    "obs_return = 200.0\n",
    "print(f\"  Raw return estimate:  ${obs_return:,.0f}\")\n",
    "print(f\"  Confidence score:     {obs_confidence:.3f}\")\n",
    "print(f\"  Adjusted return:      ${obs_confidence * obs_return:,.0f}\")\n",
    "print()\n",
    "print(\"The experiment's adjusted return is higher despite a lower raw estimate,\")\n",
    "print(\"because the stronger methodology commands greater confidence.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20",
   "metadata": {},
   "source": [
    "## Additional Resources\n",
    "\n",
    "- **Young, A. (2022)**. Consistency without inference: Instrumental variables in practical application. *European Economic Review*, 147, 104112.\n",
    "\n",
    "- **Angrist, J. D. & Pischke, J.‑S. (2010)**. The credibility revolution in empirical economics: How better research design is taking the con out of econometrics. *Journal of Economic Perspectives*, 24(2), 3–30.\n",
    "\n",
    "- [impact-engine-evaluate documentation](https://eisenhauerio.github.io/tools-impact-engine-evaluate/) — Usage, configuration, and system design"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
