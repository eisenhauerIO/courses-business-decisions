{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Evaluating Causal Evidence\n",
    "\n",
    "Measuring causal effects is only the first step. Before using an estimate to guide a business decision, we need to ask: **How much should we trust this number?** This lecture develops the conceptual tools for answering that question — from general principles of evidence quality, through diagnostic checks shared across all causal methods, to the method-specific tests that assess each design's identifying assumptions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "## 1. Evaluating Evidence in General\n",
    "\n",
    "### Internal vs. External Validity\n",
    "\n",
    "Every causal study faces two distinct questions:\n",
    "\n",
    "| Validity Type | Question | Threats |\n",
    "|---------------|----------|----------|\n",
    "| **Internal validity** | Is the causal estimate correct *for this study population*? | Selection bias, confounding, measurement error |\n",
    "| **External validity** | Does the estimate generalize *to other populations or settings*? | Sample selection, context dependence, time effects |\n",
    "\n",
    "Internal validity is a prerequisite — an estimate that is wrong for the study population tells us nothing about other settings. But a perfectly valid estimate from a narrow population may not apply elsewhere.\n",
    "\n",
    "### Statistical vs. Practical Significance\n",
    "\n",
    "A result can be statistically significant without being practically meaningful, and vice versa:\n",
    "\n",
    "- **Statistical significance** asks: Could this result have arisen by chance? (Measured by p-values, confidence intervals)\n",
    "- **Practical significance** asks: Is the effect large enough to matter for the decision? (Measured by effect sizes, cost-benefit analysis)\n",
    "\n",
    "With large samples, even trivially small effects become statistically significant. Conversely, small samples may fail to detect genuinely important effects. Decision-makers need both: evidence that the effect is real *and* that it is large enough to justify action.\n",
    "\n",
    "### Hierarchy of Evidence\n",
    "\n",
    "Not all research designs provide equally credible evidence. The strength of a causal claim depends on how effectively the design rules out alternative explanations:\n",
    "\n",
    "| Tier | Design | Strength |\n",
    "|------|--------|----------|\n",
    "| 1 | Randomized experiments (RCTs) | Random assignment eliminates selection bias by construction |\n",
    "| 2 | Quasi-experimental methods (difference-in-differences, regression discontinuity, instrumental variables) | Exploit natural variation but rely on untestable assumptions |\n",
    "| 3 | Observational methods (matching, regression) | Require strong conditional independence assumptions |\n",
    "\n",
    "Higher tiers are not always feasible. The goal is to use the strongest design available and to be transparent about the assumptions required by weaker designs.\n",
    "\n",
    "### Replication and Robustness\n",
    "\n",
    "A single estimate, no matter how well-designed the study, provides limited evidence. Confidence increases when:\n",
    "\n",
    "- **Replication**: Different studies, using different data and methods, reach similar conclusions\n",
    "- **Robustness**: The estimate remains stable across reasonable changes in specification, sample, or assumptions\n",
    "\n",
    "Sensitivity to minor analytical choices — such as which covariates to include or how to define the outcome — signals fragility in the evidence."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {},
   "source": [
    "## 2. Principles for Trustworthy Automated Assessment\n",
    "\n",
    "The diagnostic framework above applies when a human analyst reviews a study. When we automate this review — using an LLM to assess measurement artifacts and assign a confidence score — four additional failure modes emerge that must be addressed by design.\n",
    "\n",
    "### Four Failure Modes of Automated Confidence Scoring\n",
    "\n",
    "| Failure Mode | What Goes Wrong | Example |\n",
    "|---|---|---|\n",
    "| **Ungroundedness** | The score is not traceable to any observable artifact | The system produces \"confidence: 0.73\" without citing any diagnostic |\n",
    "| **Incorrectness** | Artifacts are present but misread | An unbalanced covariate table is narrated as supportive |\n",
    "| **Opacity** | No audit trail — the score cannot be challenged or inspected | \"Why 0.73 and not 0.85?\" yields a plausible but unfalsifiable answer |\n",
    "| **Instability** | Same evidence described slightly differently yields a different score | Confidence becomes a function of prompt phrasing, not measurement quality |\n",
    "\n",
    "A confidence score that cannot be defended is worse than no confidence score at all. In enterprise settings where decisions are audited and revisited, these failure modes are fatal.\n",
    "\n",
    "### Four Pillars of Defensible Confidence\n",
    "\n",
    "Each pillar directly addresses one failure mode:\n",
    "\n",
    "| Pillar | Principle | Failure Mode Addressed | Mechanism |\n",
    "|---|---|---|---|\n",
    "| **Groundedness** | Every confidence claim traces to an observable statistical artifact | Ungroundedness | Common support checks, assumption tests, robustness diagnostics |\n",
    "| **Correctness** | The system interprets those artifacts accurately | Incorrectness | Evals on synthetic artifacts with known ground truth |\n",
    "| **Traceability** | Full audit trail from score to source data | Opacity | Per-dimension scores linked to specific diagnostics |\n",
    "| **Reproducibility** | Same pipeline + same data = same assessment | Instability | Fixed prompts, structured schemas, version-pinned backends |\n",
    "\n",
    "The pillars have a natural dependency order. Groundedness is the precondition: without observable artifacts, there is nothing to be correct about. Correctness builds on groundedness: the system must read the evidence accurately. Traceability makes correctness inspectable: when interpretations are wrong, the audit trail reveals where. Reproducibility ensures all three hold across runs, not just on a single evaluation.\n",
    "\n",
    "### The LLM as Narrator, Not Oracle\n",
    "\n",
    "The key architectural implication of these pillars is a precise division of labor. The LLM's role is to *contextualize* diagnostics that the measurement engine already produces — it does not generate confidence from its own internal probabilities or invent evidence. It is a narrator reading a score sheet, not an oracle pronouncing judgment.\n",
    "\n",
    "This bounds the LLM's contribution to interpretation within a constrained evidence set, which is what makes the output auditable. The correctness of that interpretation is then the empirical question that the evaluation framework in Lecture 3 assesses. Lecture 2 examines how these four pillars are instantiated in the design patterns of a production agentic system — the registry, prompt engineering, escalation, and Assess vs. Improve discipline that together enforce the pillars in software."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3",
   "metadata": {},
   "source": [
    "## 3. Diagnostics Shared Across Causal Methods\n",
    "\n",
    "Regardless of the specific causal method, several diagnostic tools apply broadly. These checks assess whether the core assumptions of the design are plausible.\n",
    "\n",
    "### Covariate Balance\n",
    "\n",
    "Causal methods that condition on observables (matching, subclassification, regression) rely on treated and control groups being comparable. **Covariate balance** measures how similar the groups are on pre-treatment characteristics.\n",
    "\n",
    "The standard metric is the **standardized mean difference (SMD)**:\n",
    "\n",
    "$$\\text{SMD}_k = \\frac{\\bar{X}_{k,\\text{treated}} - \\bar{X}_{k,\\text{control}}}{\\sqrt{(s_{k,\\text{treated}}^2 + s_{k,\\text{control}}^2) / 2}}$$\n",
    "\n",
    "A common threshold is $|\\text{SMD}| < 0.1$, though this is a guideline, not a rule. **Love plots** display SMDs for all covariates before and after adjustment, making it easy to assess whether a method has improved balance.\n",
    "\n",
    "```{note}\n",
    "We used Love plots in the matching and subclassification lecture to assess balance improvement. The evaluate tool checks covariate balance as part of its automated review.\n",
    "```\n",
    "\n",
    "### Placebo Tests\n",
    "\n",
    "**Placebo tests** apply the causal method in settings where we know the true effect is zero. If the method detects a \"significant\" effect where none exists, something is wrong.\n",
    "\n",
    "Common placebo tests include:\n",
    "\n",
    "| Type | Description | Example |\n",
    "|------|-------------|---------|\n",
    "| **Placebo outcomes** | Apply the method to outcomes that should not be affected by treatment | Treatment is a marketing campaign; placebo outcome is warehouse temperature |\n",
    "| **Placebo treatments** | Assign treatment at a time or threshold where no real treatment occurred | In a DiD design, test for a \"treatment effect\" two periods before the actual intervention |\n",
    "| **Placebo units** | Apply the method to units that were not actually treated | In synthetic control, run the method on a control unit and check if a gap appears |\n",
    "\n",
    "### Sensitivity Analysis\n",
    "\n",
    "Every causal method relies on assumptions that cannot be fully tested with the data. **Sensitivity analysis** asks: How much would the assumptions need to be violated to overturn the conclusion?\n",
    "\n",
    "- **Rosenbaum bounds**: For matching estimators, quantify how large an unobserved confounder would need to be to explain away the effect\n",
    "- **Coefficient stability**: If adding additional covariates substantially changes the estimate, the conditional independence assumption may be fragile\n",
    "- **Omitted variable bias bounds**: Formal frameworks (e.g., Oster 2019) bound the bias from unobserved confounders based on the explanatory power of observed ones\n",
    "\n",
    "### Common Support / Overlap\n",
    "\n",
    "Methods that compare treated and control units require that both groups exist across the relevant range of covariates. **Common support** (or **overlap**) means that for any covariate value, there is a positive probability of being in either group:\n",
    "\n",
    "$$0 < P(D=1 \\mid X=x) < 1 \\quad \\text{for all } x$$\n",
    "\n",
    "Violations of common support mean the method is extrapolating rather than comparing like with like. Diagnostics include propensity score histograms and trimming rules.\n",
    "\n",
    "### Pre-treatment Outcome Trends\n",
    "\n",
    "For methods that use time-series variation (difference-in-differences, synthetic control), **parallel pre-treatment trends** between treated and control units support the identifying assumption. If outcomes diverge before treatment, the estimated effect may reflect pre-existing differences rather than the intervention."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4",
   "metadata": {},
   "source": [
    "## 4. Method-Specific Diagnostics\n",
    "\n",
    "Beyond shared diagnostics, each causal method has its own set of checks tailored to its specific assumptions.\n",
    "\n",
    "### Experiments (RCTs)\n",
    "\n",
    "| Diagnostic | What It Checks | Red Flag |\n",
    "|------------|----------------|----------|\n",
    "| **Randomization integrity** | Covariate balance between treatment and control | Systematic differences in baseline characteristics |\n",
    "| **Attrition** | Whether dropout rates differ by treatment status | Differential attrition compromises random assignment |\n",
    "| **Non-compliance** | Whether all units received their assigned treatment | High non-compliance dilutes the estimated effect |\n",
    "| **Spillover / SUTVA** | Whether treatment of one unit affects others | Interference between units biases the estimate |\n",
    "| **Effect plausibility** | Whether the magnitude of the effect is realistic | Implausibly large effects suggest measurement or specification errors |\n",
    "\n",
    "### Matching & Subclassification\n",
    "\n",
    "| Diagnostic | What It Checks | Red Flag |\n",
    "|------------|----------------|----------|\n",
    "| **Balance improvement** | Whether matching reduced covariate imbalance | SMDs remain large after matching |\n",
    "| **Common support** | Whether treated and control overlap in covariate space | Many treated units have no comparable controls |\n",
    "| **Hidden bias sensitivity** | How large an unobserved confounder would need to be | Effect disappears at low values of Rosenbaum's $\\Gamma$ |\n",
    "\n",
    "### Synthetic Control\n",
    "\n",
    "| Diagnostic | What It Checks | Red Flag |\n",
    "|------------|----------------|----------|\n",
    "| **Pre-treatment fit (RMSPE)** | How well the synthetic control tracks the treated unit before treatment | Large pre-treatment gaps undermine post-treatment comparisons |\n",
    "| **Placebo gaps** | Whether placebo units show similar post-treatment gaps | Many placebos have gaps as large as the treated unit |\n",
    "| **RMSPE ratios** | Post/pre RMSPE ratio relative to placebo distribution | Treated unit's ratio is not extreme relative to placebos |\n",
    "| **Donor composition** | Whether the synthetic control relies on sensible weights | Weights concentrated on dissimilar units |\n",
    "\n",
    "### Other Methods (Preview)\n",
    "\n",
    "Methods covered later in the course have their own diagnostics:\n",
    "\n",
    "| Method | Key Diagnostics |\n",
    "|--------|------------------|\n",
    "| **Difference-in-Differences** | Parallel pre-trends, event study plots, staggered treatment tests |\n",
    "| **Instrumental Variables** | First-stage F-statistic, exclusion restriction arguments, overidentification tests |\n",
    "| **Regression Discontinuity** | Continuity of baseline covariates at cutoff, McCrary density test, bandwidth sensitivity |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5",
   "metadata": {},
   "source": [
    "## Additional resources\n",
    "\n",
    "- **Angrist, J. D. & Pischke, J.‑S. (2010)**. The credibility revolution in empirical economics: How better research design is taking the con out of econometrics. *Journal of Economic Perspectives*, 24(2), 3–30.\n",
    "\n",
    "- **Athey, S. & Imbens, G. W. (2017)**. [The econometrics of randomized experiments](https://doi.org/10.1016/bs.hefe.2016.10.003). In *Handbook of Economic Field Experiments* (Vol. 1, pp. 73–140). Elsevier.\n",
    "\n",
    "- **Imbens, G. W. (2020)**. [Potential outcome and directed acyclic graph approaches to causality: Relevance for empirical practice in economics](https://www.aeaweb.org/articles?id=10.1257/jel.20191597). *Journal of Economic Literature*, 58(4), 1129–1179.\n",
    "\n",
    "- **Oster, E. (2019)**. Unobservable selection and coefficient stability: Theory and evidence. *Journal of Business & Economic Statistics*, 37(2), 187–204.\n",
    "\n",
    "- **Rosenbaum, P. R. (2002)**. *Observational Studies* (2nd ed.). Springer."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
